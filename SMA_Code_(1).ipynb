{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6sEhoTFDEH3"
      },
      "source": [
        "## 安裝套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V9shIFDDG3J",
        "outputId": "c380d88d-24c3-4651-a49d-221f1ba70b3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jieba in /Users/alan/anaconda3/lib/python3.10/site-packages (0.42.1)\n",
            "Requirement already satisfied: emoji in /Users/alan/anaconda3/lib/python3.10/site-packages (2.14.1)\n",
            "Requirement already satisfied: langdetect in /Users/alan/anaconda3/lib/python3.10/site-packages (1.0.9)\n",
            "Requirement already satisfied: six in /Users/alan/anaconda3/lib/python3.10/site-packages (from langdetect) (1.16.0)\n",
            "Collecting datasets\n",
            "  Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "Collecting requests>=2.32.2\n",
            "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Collecting fsspec[http]<=2025.3.0,>=2023.1.0\n",
            "  Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
            "Collecting multiprocess<0.70.17\n",
            "  Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Requirement already satisfied: pandas in /Users/alan/anaconda3/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
            "Collecting pyarrow>=15.0.0\n",
            "  Using cached pyarrow-20.0.0.tar.gz (1.1 MB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting tqdm>=4.66.3\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Collecting xxhash\n",
            "  Using cached xxhash-3.5.0-cp310-cp310-macosx_10_9_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: packaging in /Users/alan/anaconda3/lib/python3.10/site-packages (from datasets) (22.0)\n",
            "Requirement already satisfied: filelock in /Users/alan/anaconda3/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from datasets) (0.31.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/alan/anaconda3/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Using cached aiohttp-3.12.0-cp310-cp310-macosx_10_9_x86_64.whl (466 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alan/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Collecting dill<0.3.9,>=0.3.0\n",
            "  Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/alan/anaconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alan/anaconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/alan/anaconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.14)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alan/anaconda3/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
            "Collecting async-timeout<6.0,>=4.0\n",
            "  Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Using cached frozenlist-1.6.0-cp310-cp310-macosx_10_9_x86_64.whl (124 kB)\n",
            "Collecting yarl<2.0,>=1.17.0\n",
            "  Using cached yarl-1.20.0-cp310-cp310-macosx_10_9_x86_64.whl (96 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Using cached multidict-6.4.4-cp310-cp310-macosx_10_9_x86_64.whl (38 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0\n",
            "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (22.1.0)\n",
            "Collecting propcache>=0.2.0\n",
            "  Using cached propcache-0.3.1-cp310-cp310-macosx_10_9_x86_64.whl (46 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /Users/alan/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Building wheels for collected packages: pyarrow\n",
            "  Building wheel for pyarrow (pyproject.toml) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pyarrow \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[874 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/config/_apply_pyprojecttoml.py:82: SetuptoolsDeprecationWarning: `project.license` as a TOML table is deprecated\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         Please use a simple string containing a SPDX expression for `project.license`. You can also use `project.license-files`. (Both options available on setuptools>=77.0.0).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         By 2026-Feb-18, you need to update your project and remove deprecated calls\n",
            "  \u001b[31m   \u001b[0m         or your builds will no longer be supported.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   corresp(dist, value, root_dir)\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/config/_apply_pyprojecttoml.py:61: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         License :: OSI Approved :: Apache Software License\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   dist._finalize_license_expression()\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/dist.py:483: SetuptoolsDeprecationWarning: Pattern '../LICENSE.txt' cannot contain '..'\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         Please ensure the files specified are contained by the root\n",
            "  \u001b[31m   \u001b[0m         of the Python package (normally marked by `pyproject.toml`).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         By 2026-Mar-20, you need to update your project and remove deprecated calls\n",
            "  \u001b[31m   \u001b[0m         or your builds will no longer be supported.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/specifications/glob-patterns/ for details.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   for path in sorted(cls._find_pattern(pattern, enforce_match))\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/dist.py:483: SetuptoolsDeprecationWarning: Cannot find any files for the given pattern.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         Pattern '../LICENSE.txt' did not match any files.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         By 2026-Mar-20, you need to update your project and remove deprecated calls\n",
            "  \u001b[31m   \u001b[0m         or your builds will no longer be supported.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   for path in sorted(cls._find_pattern(pattern, enforce_match))\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/dist.py:483: SetuptoolsDeprecationWarning: Pattern '../NOTICE.txt' cannot contain '..'\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         Please ensure the files specified are contained by the root\n",
            "  \u001b[31m   \u001b[0m         of the Python package (normally marked by `pyproject.toml`).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         By 2026-Mar-20, you need to update your project and remove deprecated calls\n",
            "  \u001b[31m   \u001b[0m         or your builds will no longer be supported.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/specifications/glob-patterns/ for details.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   for path in sorted(cls._find_pattern(pattern, enforce_match))\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/dist.py:483: SetuptoolsDeprecationWarning: Cannot find any files for the given pattern.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         Pattern '../NOTICE.txt' did not match any files.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         By 2026-Mar-20, you need to update your project and remove deprecated calls\n",
            "  \u001b[31m   \u001b[0m         or your builds will no longer be supported.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   for path in sorted(cls._find_pattern(pattern, enforce_match))\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         License :: OSI Approved :: Apache Software License\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   self._finalize_license_expression()\n",
            "  \u001b[31m   \u001b[0m running bdist_wheel\n",
            "  \u001b[31m   \u001b[0m running build\n",
            "  \u001b[31m   \u001b[0m running build_py\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/orc.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/conftest.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_generated_version.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/benchmark.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_compute_docstrings.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/ipc.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/util.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/flight.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/cffi.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/substrait.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/types.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/dataset.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/cuda.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/feather.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/pandas_compat.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/fs.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/acero.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/csv.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/jvm.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/json.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/compute.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m running egg_info\n",
            "  \u001b[31m   \u001b[0m writing pyarrow.egg-info/PKG-INFO\n",
            "  \u001b[31m   \u001b[0m writing dependency_links to pyarrow.egg-info/dependency_links.txt\n",
            "  \u001b[31m   \u001b[0m writing requirements to pyarrow.egg-info/requires.txt\n",
            "  \u001b[31m   \u001b[0m writing top-level names to pyarrow.egg-info/top_level.txt\n",
            "  \u001b[31m   \u001b[0m ERROR setuptools_scm._file_finders.git listing git files failed - pretending there aren't any\n",
            "  \u001b[31m   \u001b[0m reading manifest file 'pyarrow.egg-info/SOURCES.txt'\n",
            "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
            "  \u001b[31m   \u001b[0m warning: no files found matching '../LICENSE.txt'\n",
            "  \u001b[31m   \u001b[0m warning: no files found matching '../NOTICE.txt'\n",
            "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*.so' found anywhere in distribution\n",
            "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
            "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*~' found anywhere in distribution\n",
            "  \u001b[31m   \u001b[0m warning: no previously-included files matching '#*' found anywhere in distribution\n",
            "  \u001b[31m   \u001b[0m warning: no previously-included files matching '.git*' found anywhere in distribution\n",
            "  \u001b[31m   \u001b[0m warning: no previously-included files matching '.DS_Store' found anywhere in distribution\n",
            "  \u001b[31m   \u001b[0m no previously-included directories found matching '.asv'\n",
            "  \u001b[31m   \u001b[0m writing manifest file 'pyarrow.egg-info/SOURCES.txt'\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.includes' is absent from the `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.includes' as an importable package[^1],\n",
            "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.includes' is explicitly added\n",
            "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
            "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.includes' to be distributed and are\n",
            "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.includes' via\n",
            "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
            "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
            "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.interchange' is absent from the `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.interchange' as an importable package[^1],\n",
            "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.interchange' is explicitly added\n",
            "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
            "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.interchange' to be distributed and are\n",
            "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.interchange' via\n",
            "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
            "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
            "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.parquet' is absent from the `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.parquet' as an importable package[^1],\n",
            "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.parquet' is explicitly added\n",
            "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
            "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.parquet' to be distributed and are\n",
            "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.parquet' via\n",
            "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
            "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
            "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.src.arrow.python' is absent from the `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.src.arrow.python' as an importable package[^1],\n",
            "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.src.arrow.python' is explicitly added\n",
            "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
            "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.src.arrow.python' to be distributed and are\n",
            "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.src.arrow.python' via\n",
            "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
            "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
            "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.src.arrow.python.vendored' is absent from the `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.src.arrow.python.vendored' as an importable package[^1],\n",
            "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.src.arrow.python.vendored' is explicitly added\n",
            "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
            "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.src.arrow.python.vendored' to be distributed and are\n",
            "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.src.arrow.python.vendored' via\n",
            "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
            "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
            "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.tests' is absent from the `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.tests' as an importable package[^1],\n",
            "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.tests' is explicitly added\n",
            "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
            "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.tests' to be distributed and are\n",
            "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.tests' via\n",
            "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
            "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
            "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.tests.data.feather' is absent from the `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.tests.data.feather' as an importable package[^1],\n",
            "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.tests.data.feather' is explicitly added\n",
            "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
            "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.tests.data.feather' to be distributed and are\n",
            "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.tests.data.feather' via\n",
            "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
            "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
            "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.tests.data.orc' is absent from the `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.tests.data.orc' as an importable package[^1],\n",
            "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.tests.data.orc' is explicitly added\n",
            "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
            "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.tests.data.orc' to be distributed and are\n",
            "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.tests.data.orc' via\n",
            "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
            "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
            "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.tests.data.parquet' is absent from the `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.tests.data.parquet' as an importable package[^1],\n",
            "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.tests.data.parquet' is explicitly added\n",
            "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
            "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.tests.data.parquet' to be distributed and are\n",
            "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.tests.data.parquet' via\n",
            "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
            "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
            "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.tests.interchange' is absent from the `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.tests.interchange' as an importable package[^1],\n",
            "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.tests.interchange' is explicitly added\n",
            "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
            "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.tests.interchange' to be distributed and are\n",
            "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.tests.interchange' via\n",
            "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
            "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
            "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.tests.parquet' is absent from the `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.tests.parquet' as an importable package[^1],\n",
            "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.tests.parquet' is explicitly added\n",
            "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
            "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.tests.parquet' to be distributed and are\n",
            "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.tests.parquet' via\n",
            "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
            "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
            "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-build-env-k0793fzj/overlay/lib/python3.10/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.vendored' is absent from the `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
            "  \u001b[31m   \u001b[0m         ############################\n",
            "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.vendored' as an importable package[^1],\n",
            "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.vendored' is explicitly added\n",
            "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
            "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.vendored' to be distributed and are\n",
            "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.vendored' via\n",
            "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
            "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
            "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
            "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m !!\n",
            "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/__init__.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_acero.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_acero.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_azurefs.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_compute.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_compute.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_csv.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_csv.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_cuda.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_cuda.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_dataset.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_dataset.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_dataset_orc.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_dataset_parquet.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_dataset_parquet.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_dataset_parquet_encryption.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_dlpack.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_feather.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_flight.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_fs.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_fs.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_gcsfs.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_hdfs.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_json.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_json.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_orc.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_orc.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_parquet.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_parquet.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_parquet_encryption.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_parquet_encryption.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_pyarrow_cpp_tests.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_pyarrow_cpp_tests.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_s3fs.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/_substrait.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/array.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/benchmark.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/builder.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/compat.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/config.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/device.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/error.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/gandiva.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/io.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/ipc.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/lib.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/lib.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/memory.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/pandas-shim.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/public-api.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/scalar.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/table.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tensor.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/types.pxi -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/includes/__init__.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/includes/common.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_acero.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_cuda.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_dataset.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_dataset_parquet.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_feather.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_flight.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_fs.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_python.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_substrait.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/includes/libgandiva.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/includes/libparquet_encryption.pxd -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/includes\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/interchange\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/interchange/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/interchange\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/interchange/buffer.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/interchange\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/interchange/column.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/interchange\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/interchange/dataframe.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/interchange\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/interchange/from_dataframe.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/interchange\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/parquet/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/parquet/core.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/parquet/encryption.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/parquet\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/CMakeLists.txt -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/api.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/arrow_to_pandas.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/arrow_to_pandas.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/arrow_to_python_internal.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/async.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/benchmark.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/benchmark.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/common.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/common.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/csv.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/csv.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/datetime.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/datetime.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/decimal.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/decimal.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/extension_type.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/extension_type.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/filesystem.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/filesystem.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/flight.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/flight.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/gdb.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/gdb.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/helpers.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/helpers.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/inference.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/inference.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/io.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/io.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/ipc.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/ipc.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/iterators.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_convert.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_convert.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_init.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_init.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_internal.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_interop.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_to_arrow.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_to_arrow.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/parquet_encryption.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/parquet_encryption.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/pch.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/platform.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/pyarrow.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/pyarrow.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/pyarrow_api.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/pyarrow_lib.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/python_test.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/python_test.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/python_to_arrow.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/python_to_arrow.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/type_traits.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/udf.cc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/udf.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/visibility.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python/vendored\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/vendored/CMakeLists.txt -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python/vendored\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/vendored/pythoncapi_compat.h -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/src/arrow/python/vendored\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/arrow_16597.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/arrow_39313.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/arrow_7980.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/bound_function_visit_strings.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/conftest.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/extensions.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/pandas_examples.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/pandas_threaded_import.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/pyarrow_cython_example.pyx -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/read_record_batch.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/strategies.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_acero.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_adhoc_memory_leak.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_array.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_builder.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_cffi.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_compute.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_convert_builtin.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_cpp_internals.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_csv.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_cuda.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_cuda_numba_interop.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_cython.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_dataset.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_dataset_encryption.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_deprecations.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_device.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_dlpack.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_exec_plan.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_extension_type.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_feather.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_flight.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_flight_async.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_fs.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_gandiva.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_gdb.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_io.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_ipc.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_json.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_jvm.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_memory.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_misc.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_orc.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_pandas.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_scalars.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_schema.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_sparse_tensor.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_strategies.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_substrait.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_table.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_tensor.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_types.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_udf.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_util.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_without_numpy.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/util.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/wsgi_examples.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/feather\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/feather/v0.17.0.version.2-compression.lz4.feather -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/feather\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/orc\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/README.md -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/orc\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.jsn.gz -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/orc\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.orc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/orc\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/TestOrcFile.test1.jsn.gz -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/orc\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/TestOrcFile.test1.orc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/orc\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.jsn.gz -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/orc\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.orc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/orc\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/decimal.jsn.gz -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/orc\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/decimal.orc -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/orc\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/parquet/v0.7.1.all-named-index.parquet -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/parquet/v0.7.1.column-metadata-handling.parquet -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/parquet/v0.7.1.parquet -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/parquet/v0.7.1.some-named-index.parquet -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/data/parquet\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/interchange\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/interchange/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/interchange\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/interchange/test_conversion.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/interchange\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/interchange/test_interchange_spec.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/interchange\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/common.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/conftest.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/encryption.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_basic.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_compliant_nested_type.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_data_types.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_dataset.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_datetime.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_encryption.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_metadata.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_pandas.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_parquet_file.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_parquet_writer.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/tests/parquet\n",
            "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/vendored\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/vendored/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/vendored\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/vendored/docscrape.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/vendored\n",
            "  \u001b[31m   \u001b[0m copying pyarrow/vendored/version.py -> build/lib.macosx-10.9-x86_64-cpython-310/pyarrow/vendored\n",
            "  \u001b[31m   \u001b[0m running build_ext\n",
            "  \u001b[31m   \u001b[0m creating /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-install-g0hjcmu8/pyarrow_91c213923b9e4a728fddb899f8e9c208/build/temp.macosx-10.9-x86_64-cpython-310\n",
            "  \u001b[31m   \u001b[0m -- Running cmake for PyArrow\n",
            "  \u001b[31m   \u001b[0m cmake -DCMAKE_INSTALL_PREFIX=/private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-install-g0hjcmu8/pyarrow_91c213923b9e4a728fddb899f8e9c208/build/lib.macosx-10.9-x86_64-cpython-310/pyarrow -DPYTHON_EXECUTABLE=/Users/alan/anaconda3/bin/python -DPython3_EXECUTABLE=/Users/alan/anaconda3/bin/python -DPYARROW_CXXFLAGS= -DPYARROW_BUNDLE_ARROW_CPP=off -DPYARROW_BUNDLE_CYTHON_CPP=off -DPYARROW_GENERATE_COVERAGE=off -DCMAKE_BUILD_TYPE=release /private/var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/pip-install-g0hjcmu8/pyarrow_91c213923b9e4a728fddb899f8e9c208\n",
            "  \u001b[31m   \u001b[0m error: command 'cmake' failed: No such file or directory\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for pyarrow\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build pyarrow\n",
            "\u001b[31mERROR: Could not build wheels for pyarrow, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: lingua-language-detector in /Users/alan/anaconda3/lib/python3.10/site-packages (2.1.0)\n",
            "Requirement already satisfied: lime in /Users/alan/anaconda3/lib/python3.10/site-packages (0.2.0.1)\n",
            "Requirement already satisfied: shap in /Users/alan/anaconda3/lib/python3.10/site-packages (0.47.2)\n",
            "Requirement already satisfied: snownlp in /Users/alan/anaconda3/lib/python3.10/site-packages (0.12.3)\n",
            "Requirement already satisfied: textblob in /Users/alan/anaconda3/lib/python3.10/site-packages (0.19.0)\n",
            "Requirement already satisfied: text2emotion in /Users/alan/anaconda3/lib/python3.10/site-packages (0.0.5)\n",
            "Requirement already satisfied: hanlp in /Users/alan/anaconda3/lib/python3.10/site-packages (2.1.1)\n",
            "Requirement already satisfied: spaCy in /Users/alan/anaconda3/lib/python3.10/site-packages (3.8.7)\n",
            "Requirement already satisfied: numpy in /Users/alan/anaconda3/lib/python3.10/site-packages (from lime) (1.23.5)\n",
            "Requirement already satisfied: scipy in /Users/alan/anaconda3/lib/python3.10/site-packages (from lime) (1.10.0)\n",
            "Requirement already satisfied: tqdm in /Users/alan/anaconda3/lib/python3.10/site-packages (from lime) (4.64.1)\n",
            "Requirement already satisfied: matplotlib in /Users/alan/anaconda3/lib/python3.10/site-packages (from lime) (3.7.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /Users/alan/anaconda3/lib/python3.10/site-packages (from lime) (0.19.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /Users/alan/anaconda3/lib/python3.10/site-packages (from lime) (1.2.1)\n",
            "Requirement already satisfied: numba>=0.54 in /Users/alan/anaconda3/lib/python3.10/site-packages (from shap) (0.56.4)\n",
            "Requirement already satisfied: slicer==0.0.8 in /Users/alan/anaconda3/lib/python3.10/site-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions in /Users/alan/anaconda3/lib/python3.10/site-packages (from shap) (4.13.2)\n",
            "Requirement already satisfied: packaging>20.9 in /Users/alan/anaconda3/lib/python3.10/site-packages (from shap) (22.0)\n",
            "Requirement already satisfied: pandas in /Users/alan/anaconda3/lib/python3.10/site-packages (from shap) (2.0.3)\n",
            "Requirement already satisfied: cloudpickle in /Users/alan/anaconda3/lib/python3.10/site-packages (from shap) (2.0.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /Users/alan/anaconda3/lib/python3.10/site-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: emoji>=0.6.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from text2emotion) (2.14.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from hanlp) (1.13.1)\n",
            "Requirement already satisfied: hanlp-trie>=0.0.4 in /Users/alan/anaconda3/lib/python3.10/site-packages (from hanlp) (0.0.5)\n",
            "Requirement already satisfied: hanlp-common>=0.0.23 in /Users/alan/anaconda3/lib/python3.10/site-packages (from hanlp) (0.0.23)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /Users/alan/anaconda3/lib/python3.10/site-packages (from hanlp) (0.2.0)\n",
            "Requirement already satisfied: toposort==1.5 in /Users/alan/anaconda3/lib/python3.10/site-packages (from hanlp) (1.5)\n",
            "Requirement already satisfied: transformers>=4.1.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from hanlp) (4.24.0)\n",
            "Requirement already satisfied: pynvml in /Users/alan/anaconda3/lib/python3.10/site-packages (from hanlp) (12.0.0)\n",
            "Requirement already satisfied: termcolor in /Users/alan/anaconda3/lib/python3.10/site-packages (from hanlp) (2.3.0)\n",
            "Requirement already satisfied: hanlp-downloader in /Users/alan/anaconda3/lib/python3.10/site-packages (from hanlp) (0.0.25)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (1.10.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (2.0.11)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (3.0.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (2.28.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (2.5.1)\n",
            "Requirement already satisfied: jinja2 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (3.1.2)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (8.3.4)\n",
            "Requirement already satisfied: setuptools in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (65.6.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (1.0.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (3.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (2.0.10)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (3.0.12)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (1.0.13)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from spaCy) (0.15.4)\n",
            "Requirement already satisfied: phrasetree>=0.0.9 in /Users/alan/anaconda3/lib/python3.10/site-packages (from hanlp-common>=0.0.23->hanlp) (0.0.9)\n",
            "Requirement already satisfied: language-data>=1.2 in /Users/alan/anaconda3/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spaCy) (1.3.0)\n",
            "Requirement already satisfied: joblib in /Users/alan/anaconda3/lib/python3.10/site-packages (from nltk>=3.9->textblob) (1.1.1)\n",
            "Requirement already satisfied: click in /Users/alan/anaconda3/lib/python3.10/site-packages (from nltk>=3.9->textblob) (8.0.4)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/alan/anaconda3/lib/python3.10/site-packages (from nltk>=3.9->textblob) (2022.7.9)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from numba>=0.54->shap) (0.39.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/alan/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2023.7.22)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/alan/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/alan/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.0.4)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (2.26.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /Users/alan/anaconda3/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (2.8.4)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (9.4.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /Users/alan/anaconda3/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (2021.7.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from scikit-learn>=0.18->lime) (2.2.0)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spaCy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spaCy) (0.1.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from transformers>=4.1.1->hanlp) (0.31.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from transformers>=4.1.1->hanlp) (6.0)\n",
            "Requirement already satisfied: filelock in /Users/alan/anaconda3/lib/python3.10/site-packages (from transformers>=4.1.1->hanlp) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from transformers>=4.1.1->hanlp) (0.11.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spaCy) (14.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spaCy) (1.5.4)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spaCy) (5.2.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spaCy) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from jinja2->spaCy) (2.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/alan/anaconda3/lib/python3.10/site-packages (from matplotlib->lime) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from matplotlib->lime) (1.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/alan/anaconda3/lib/python3.10/site-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from matplotlib->lime) (4.25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from matplotlib->lime) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from matplotlib->lime) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from pandas->shap) (2022.7)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from pandas->shap) (2023.3)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from pynvml->hanlp) (12.575.51)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers>=4.1.1->hanlp) (1.1.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers>=4.1.1->hanlp) (2025.3.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spaCy) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/alan/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/alan/anaconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (0.1.2)\n",
            "Requirement already satisfied: opencc-python-reimplemented in /Users/alan/anaconda3/lib/python3.10/site-packages (0.1.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install jieba\n",
        "!pip install emoji langdetect\n",
        "!pip install datasets\n",
        "!pip install lingua-language-detector lime shap snownlp textblob text2emotion hanlp spaCy spacy\n",
        "!pip install opencc-python-reimplemented\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhtPvgrLCUzp",
        "outputId": "69d404de-1ede-4764-cc57-77c5e66bdceb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jieba in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.42.1)\n",
            "Requirement already satisfied: emoji in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.14.1)\n",
            "Requirement already satisfied: langdetect in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.0.9)\n",
            "Requirement already satisfied: pytz in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2025.1)\n",
            "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.2)\n",
            "Requirement already satisfied: lingua-language-detector in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.1.0)\n",
            "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.6.0)\n",
            "Requirement already satisfied: six in /Users/alan/Library/Python/3.10/lib/python/site-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /Users/alan/Library/Python/3.10/lib/python/site-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.1.5)\n",
            "Requirement already satisfied: lime in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.2.0.1)\n",
            "Requirement already satisfied: shap in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.47.2)\n",
            "Requirement already satisfied: snownlp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.12.3)\n",
            "Requirement already satisfied: textblob in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.19.0)\n",
            "Requirement already satisfied: hanlp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.1.1)\n",
            "Requirement already satisfied: spaCy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.8.7)\n",
            "Requirement already satisfied: text2emotion in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.0.5)\n",
            "Requirement already satisfied: opencc-python-reimplemented in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.1.7)\n",
            "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lime) (3.10.1)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lime) (1.26.4)\n",
            "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lime) (1.15.2)\n",
            "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from shap) (2.2.3)\n",
            "Requirement already satisfied: packaging>20.9 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from shap) (24.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from shap) (0.61.2)\n",
            "Requirement already satisfied: cloudpickle in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from shap) (4.13.2)\n",
            "Requirement already satisfied: nltk>=3.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: hanlp-common>=0.0.23 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp) (0.0.23)\n",
            "Requirement already satisfied: hanlp-downloader in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp) (0.0.25)\n",
            "Requirement already satisfied: hanlp-trie>=0.0.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp) (0.0.5)\n",
            "Requirement already satisfied: pynvml in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp) (12.0.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp) (0.2.0)\n",
            "Requirement already satisfied: termcolor in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp) (3.1.0)\n",
            "Requirement already satisfied: toposort==1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp) (1.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp) (2.2.2)\n",
            "Requirement already satisfied: transformers>=4.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp) (4.51.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (0.15.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from spaCy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (65.5.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spaCy) (3.5.0)\n",
            "Requirement already satisfied: emoji>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from text2emotion) (2.14.1)\n",
            "Requirement already satisfied: phrasetree>=0.0.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp-common>=0.0.23->hanlp) (0.0.9)\n",
            "Requirement already satisfied: language-data>=1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spaCy) (1.3.0)\n",
            "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from numba>=0.54->shap) (0.44.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2024.2.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (11.1.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (2025.5.10)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spaCy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spaCy) (0.1.5)\n",
            "Collecting numpy (from lime)\n",
            "  Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.6.0->hanlp) (3.18.0)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.6.0->hanlp) (1.13.3)\n",
            "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.6.0->hanlp) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=4.1.1->hanlp) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from transformers>=4.1.1->hanlp) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=4.1.1->hanlp) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=4.1.1->hanlp) (0.5.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spaCy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spaCy) (14.0.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spaCy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spaCy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from jinja2->spaCy) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->lime) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->shap) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pynvml->hanlp) (12.575.51)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spaCy) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (2.17.2)\n",
            "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spaCy) (1.17.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch>=1.6.0->hanlp) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (0.1.2)\n",
            "Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl (5.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-macos 2.13.1 requires numpy<=1.24.3,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "tensorflow-macos 2.13.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.6\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install jieba emoji langdetect pytz torch lingua-language-detector datasets \n",
        "!{sys.executable} -m pip install openpyxl lime shap snownlp textblob hanlp spaCy textblob text2emotion opencc-python-reimplemented\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HGttCcMDKE0"
      },
      "source": [
        "## 引入套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84LHS4XIuqYL",
        "outputId": "a6530f44-9afb-4942-ed00-619c1c4caa09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests->transformers) (2024.2.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import jieba\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import emoji\n",
        "import langdetect\n",
        "from langdetect import detect\n",
        "from lingua import LanguageDetectorBuilder, Language, IsoCode639_1\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM7PgPNTPXMd",
        "outputId": "94b15d34-4800-47c7-a22f-f7ac7081f7e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # 設定好路徑 (後面都是使用相對路徑)\n",
        "# base_path = '/content/drive/My Drive/SMA'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xOIyJJ3KSNT9",
        "outputId": "af538717-6768-4634-9cc1-8d7f36c21bf7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>post_time</th>\n",
              "      <th>topic</th>\n",
              "      <th>time_info</th>\n",
              "      <th>content</th>\n",
              "      <th>has_photo</th>\n",
              "      <th>has_video</th>\n",
              "      <th>like_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>repost_count</th>\n",
              "      <th>share_count</th>\n",
              "      <th>view_count</th>\n",
              "      <th>followers_count</th>\n",
              "      <th>post_url</th>\n",
              "      <th>scrape_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ayofvr</td>\n",
              "      <td>2025-04-29T10:58:39.000Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3小時</td>\n",
              "      <td>Thank you God for another day.</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>190</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3,197</td>\n",
              "      <td>141073</td>\n",
              "      <td>https://www.threads.net/@ayofvr/post/DJBymf8uTrK</td>\n",
              "      <td>2025-04-29T22:27:40.176749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2025-04-29T12:33:44.000Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1小時</td>\n",
              "      <td>百達翡麗？ 沒有下限的網路病態！</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>196</td>\n",
              "      <td>16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>3 萬</td>\n",
              "      <td>77683</td>\n",
              "      <td>https://www.threads.net/@ban.mei.onnnnni/post/...</td>\n",
              "      <td>2025-04-29T22:27:54.964788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ribboworld2021</td>\n",
              "      <td>2025-04-29T04:39:46.000Z</td>\n",
              "      <td>小一日常</td>\n",
              "      <td>9小時</td>\n",
              "      <td>考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>75</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4,559</td>\n",
              "      <td>99</td>\n",
              "      <td>https://www.threads.net/@ribboworld2021/post/D...</td>\n",
              "      <td>2025-04-29T22:28:09.873641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ayofvr</td>\n",
              "      <td>2025-04-29T11:25:22.000Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3小時</td>\n",
              "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>83</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1,967</td>\n",
              "      <td>141093</td>\n",
              "      <td>https://www.threads.net/@ayofvr/post/DJB1qP5OmzP</td>\n",
              "      <td>2025-04-29T22:28:24.726576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jose_ykc</td>\n",
              "      <td>2025-04-29T10:36:19.000Z</td>\n",
              "      <td>輔仁大學</td>\n",
              "      <td>3小時</td>\n",
              "      <td>日文輔系老師上課內容之一AiScReam 歌詞導讀</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>4,334</td>\n",
              "      <td>55</td>\n",
              "      <td>513</td>\n",
              "      <td>2,460</td>\n",
              "      <td>10 萬</td>\n",
              "      <td>65</td>\n",
              "      <td>https://www.threads.net/@jose_ykc/post/DJBvpGI...</td>\n",
              "      <td>2025-04-29T22:28:39.393706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3096</th>\n",
              "      <td>leighton.williams</td>\n",
              "      <td>2025年04月29日 07:30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8小時</td>\n",
              "      <td>First tasting in California</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>726</td>\n",
              "      <td>30</td>\n",
              "      <td>https://www.threads.net/@leighton.williams/pos...</td>\n",
              "      <td>2025-04-29T16:23:40.328046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3097</th>\n",
              "      <td>cape__man</td>\n",
              "      <td>2025年04月29日 06:31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9小時</td>\n",
              "      <td>I hoped it would have been better.</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1,291</td>\n",
              "      <td>114</td>\n",
              "      <td>https://www.threads.net/@cape__man/post/DJAdFd...</td>\n",
              "      <td>2025-04-29T16:23:55.063517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3098</th>\n",
              "      <td>simimoonlight</td>\n",
              "      <td>2025年04月29日 06:26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9小時</td>\n",
              "      <td>I can’t wait to watch Beyoncé on TikTok tonigh...</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>2,656</td>\n",
              "      <td>29</td>\n",
              "      <td>280</td>\n",
              "      <td>47</td>\n",
              "      <td>5.6 萬</td>\n",
              "      <td>65732</td>\n",
              "      <td>https://www.threads.net/@simimoonlight/post/DJ...</td>\n",
              "      <td>2025-04-29T16:24:39.870994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3099</th>\n",
              "      <td>other98</td>\n",
              "      <td>2025年04月29日 12:31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3小時</td>\n",
              "      <td>Tonight, Canada just proved that they have a h...</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>1.0 萬</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>22</td>\n",
              "      <td>14 萬</td>\n",
              "      <td>217182</td>\n",
              "      <td>https://www.threads.net/@other98/post/DJBGV3NxiX_</td>\n",
              "      <td>2025-04-29T16:24:54.669936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3100</th>\n",
              "      <td>scottiebeam</td>\n",
              "      <td>2025年04月29日 01:33</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14小時</td>\n",
              "      <td>Yall be fighting on here … i thought threads w...</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>427</td>\n",
              "      <td>76</td>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>1 萬</td>\n",
              "      <td>59249</td>\n",
              "      <td>https://www.threads.net/@scottiebeam/post/DI_6...</td>\n",
              "      <td>2025-04-29T16:25:09.396585</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3101 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 author                 post_time topic time_info  \\\n",
              "0                ayofvr  2025-04-29T10:58:39.000Z   NaN       3小時   \n",
              "1                   NaN  2025-04-29T12:33:44.000Z   NaN       1小時   \n",
              "2        ribboworld2021  2025-04-29T04:39:46.000Z  小一日常       9小時   \n",
              "3                ayofvr  2025-04-29T11:25:22.000Z   NaN       3小時   \n",
              "4              jose_ykc  2025-04-29T10:36:19.000Z  輔仁大學       3小時   \n",
              "...                 ...                       ...   ...       ...   \n",
              "3096  leighton.williams         2025年04月29日 07:30   NaN       8小時   \n",
              "3097          cape__man         2025年04月29日 06:31   NaN       9小時   \n",
              "3098      simimoonlight         2025年04月29日 06:26   NaN       9小時   \n",
              "3099            other98         2025年04月29日 12:31   NaN       3小時   \n",
              "3100        scottiebeam         2025年04月29日 01:33   NaN      14小時   \n",
              "\n",
              "                                                content has_photo has_video  \\\n",
              "0                        Thank you God for another day.         N         N   \n",
              "1                                      百達翡麗？ 沒有下限的網路病態！         Y         N   \n",
              "2              考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」         N         N   \n",
              "3     Just be strong. Confident. Hopeful. Intellectu...         N         N   \n",
              "4                             日文輔系老師上課內容之一AiScReam 歌詞導讀         Y         Y   \n",
              "...                                                 ...       ...       ...   \n",
              "3096                        First tasting in California         N         N   \n",
              "3097                 I hoped it would have been better.         N         N   \n",
              "3098  I can’t wait to watch Beyoncé on TikTok tonigh...         N         N   \n",
              "3099  Tonight, Canada just proved that they have a h...         N         N   \n",
              "3100  Yall be fighting on here … i thought threads w...         N         N   \n",
              "\n",
              "     like_count reply_count repost_count share_count view_count  \\\n",
              "0           190           3           23         NaN      3,197   \n",
              "1           196          16          NaN           6        3 萬   \n",
              "2            75           6          NaN         NaN      4,559   \n",
              "3            83           5            3           1      1,967   \n",
              "4         4,334          55          513       2,460       10 萬   \n",
              "...         ...         ...          ...         ...        ...   \n",
              "3096          7         NaN          NaN           0        726   \n",
              "3097          2         NaN          NaN           0      1,291   \n",
              "3098      2,656          29          280          47      5.6 萬   \n",
              "3099      1.0 萬         214          214          22       14 萬   \n",
              "3100        427          76           28           1        1 萬   \n",
              "\n",
              "      followers_count                                           post_url  \\\n",
              "0              141073   https://www.threads.net/@ayofvr/post/DJBymf8uTrK   \n",
              "1               77683  https://www.threads.net/@ban.mei.onnnnni/post/...   \n",
              "2                  99  https://www.threads.net/@ribboworld2021/post/D...   \n",
              "3              141093   https://www.threads.net/@ayofvr/post/DJB1qP5OmzP   \n",
              "4                  65  https://www.threads.net/@jose_ykc/post/DJBvpGI...   \n",
              "...               ...                                                ...   \n",
              "3096               30  https://www.threads.net/@leighton.williams/pos...   \n",
              "3097              114  https://www.threads.net/@cape__man/post/DJAdFd...   \n",
              "3098            65732  https://www.threads.net/@simimoonlight/post/DJ...   \n",
              "3099           217182  https://www.threads.net/@other98/post/DJBGV3NxiX_   \n",
              "3100            59249  https://www.threads.net/@scottiebeam/post/DI_6...   \n",
              "\n",
              "                     scrape_time  \n",
              "0     2025-04-29T22:27:40.176749  \n",
              "1     2025-04-29T22:27:54.964788  \n",
              "2     2025-04-29T22:28:09.873641  \n",
              "3     2025-04-29T22:28:24.726576  \n",
              "4     2025-04-29T22:28:39.393706  \n",
              "...                          ...  \n",
              "3096  2025-04-29T16:23:40.328046  \n",
              "3097  2025-04-29T16:23:55.063517  \n",
              "3098  2025-04-29T16:24:39.870994  \n",
              "3099  2025-04-29T16:24:54.669936  \n",
              "3100  2025-04-29T16:25:09.396585  \n",
              "\n",
              "[3101 rows x 15 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# 讀取資料（請確認你的 Excel 路徑）\n",
        "# df = pd.read_excel(base_path+\"/threads.xlsx\")\n",
        "df = pd.read_excel(\"threads.xlsx\", engine='openpyxl')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "dPGT6_oP19QF",
        "outputId": "3739ffb2-a43e-4ef6-e7f1-325118a951d8"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# import pandas as pd\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# import io\n",
        "# file_name = list(uploaded.keys())[0]\n",
        "# df = pd.read_excel(io.BytesIO(uploaded[file_name]), engine='openpyxl')\n",
        "\n",
        "# df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kIM9eoPtiDSC"
      },
      "outputs": [],
      "source": [
        "# === 語言偵測修正版===\n",
        "lingua_detector = LanguageDetectorBuilder.from_all_languages().with_preloaded_language_models().build()\n",
        "lingua_available = True\n",
        "def detect_lang_with_preprocessing_lingua(text):\n",
        "    original_text = text\n",
        "\n",
        "    # 若是 NaN 或空字串就回傳 \"unknown\"\n",
        "    if pd.isna(text):\n",
        "        return \"unknown\"\n",
        "    text = str(text).strip()\n",
        "    if not text:\n",
        "        return \"unknown\"\n",
        "\n",
        "    # 移除 URL、@標記、 #hashtag、emoji、多餘空白\n",
        "    try:\n",
        "      text_cleaned = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "      text_cleaned = re.sub(r'@\\w+', '', text_cleaned)\n",
        "      text_cleaned = re.sub(r'#\\w+', '', text_cleaned)\n",
        "      text_cleaned = emoji.replace_emoji(text_cleaned, replace='')\n",
        "      text_cleaned = re.sub(r'\\s+', ' ', text_cleaned).strip()\n",
        "    except Exception as e:\n",
        "      return \"error_state_preprocessing\"\n",
        "\n",
        "    # 若這些清理完後變成空字串\n",
        "    if not text_cleaned:\n",
        "      return \"empty_after_clean\"\n",
        "\n",
        "    # 若文字中超過 30% 是中文，就直接判定為 \"Ch\"（中文）\n",
        "    try:\n",
        "      chinese_chars = re.findall(r'[\\u4e00-\\u9fff]', text_cleaned)\n",
        "      text_len = len(text_cleaned)\n",
        "      ratio = len(chinese_chars) / max(text_len, 1)\n",
        "      chinese_threshold = 0.3\n",
        "      if ratio > chinese_threshold:\n",
        "        return \"Ch\"\n",
        "\n",
        "      # 呼叫 lingua 偵測語言\n",
        "      detected_language = lingua_detector.detect_language_of(text_cleaned)\n",
        "\n",
        "      # 若 lingua 判定是中文（'ZH'），則回傳 \"Ch\"，其餘語言以小寫的 ISO 639-1 回傳（如 en, ja, fr）\n",
        "      # 若無法偵測出語言，回傳 \"unknown\"\n",
        "      if detected_language is not None:\n",
        "        iso_code = detected_language.iso_code_639_1.name\n",
        "        if iso_code == 'ZH':\n",
        "          return \"Ch\"\n",
        "        else:\n",
        "          return iso_code.lower()\n",
        "      else:\n",
        "        return \"unknown\"\n",
        "\n",
        "    except Exception as e:\n",
        "      return \"unknown\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_nqIl5HD2ia"
      },
      "source": [
        "## 清洗數據V1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LsfgBIPFrsc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 處理完成，已輸出 threads_cleaned_v1.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>post_time</th>\n",
              "      <th>topic</th>\n",
              "      <th>time_info</th>\n",
              "      <th>content</th>\n",
              "      <th>has_photo</th>\n",
              "      <th>has_video</th>\n",
              "      <th>like_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>repost_count</th>\n",
              "      <th>...</th>\n",
              "      <th>scrape_time</th>\n",
              "      <th>emojis</th>\n",
              "      <th>emoji_count</th>\n",
              "      <th>lang</th>\n",
              "      <th>scrape_time_origin</th>\n",
              "      <th>post_weekday</th>\n",
              "      <th>post_hour</th>\n",
              "      <th>viral</th>\n",
              "      <th>has_question</th>\n",
              "      <th>has_exclaim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ayofvr</td>\n",
              "      <td>2025-04-29T10:58:39.000Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3小時</td>\n",
              "      <td>Thank you God for another day.</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>190</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>...</td>\n",
              "      <td>2025年04月30日 06:27</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "      <td>2025-04-30 06:27:40.176749+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2025-04-29T12:33:44.000Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1小時</td>\n",
              "      <td>百達翡麗？ 沒有下限的網路病態！</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>196</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2025年04月30日 06:27</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>Ch</td>\n",
              "      <td>2025-04-30 06:27:54.964788+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ribboworld2021</td>\n",
              "      <td>2025-04-29T04:39:46.000Z</td>\n",
              "      <td>小一日常</td>\n",
              "      <td>9小時</td>\n",
              "      <td>考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>75</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2025年04月30日 06:28</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>Ch</td>\n",
              "      <td>2025-04-30 06:28:09.873641+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ayofvr</td>\n",
              "      <td>2025-04-29T11:25:22.000Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3小時</td>\n",
              "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>83</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>2025年04月30日 06:28</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "      <td>2025-04-30 06:28:24.726576+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jose_ykc</td>\n",
              "      <td>2025-04-29T10:36:19.000Z</td>\n",
              "      <td>輔仁大學</td>\n",
              "      <td>3小時</td>\n",
              "      <td>日文輔系老師上課內容之一AiScReam 歌詞導讀</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>4334</td>\n",
              "      <td>55</td>\n",
              "      <td>513</td>\n",
              "      <td>...</td>\n",
              "      <td>2025年04月30日 06:28</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>Ch</td>\n",
              "      <td>2025-04-30 06:28:39.393706+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3096</th>\n",
              "      <td>leighton.williams</td>\n",
              "      <td>2025年04月29日 07:30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8小時</td>\n",
              "      <td>First tasting in California</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2025年04月30日 00:23</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "      <td>2025-04-30 00:23:40.328046+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3097</th>\n",
              "      <td>cape__man</td>\n",
              "      <td>2025年04月29日 06:31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9小時</td>\n",
              "      <td>I hoped it would have been better.</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2025年04月30日 00:23</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "      <td>2025-04-30 00:23:55.063517+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3098</th>\n",
              "      <td>simimoonlight</td>\n",
              "      <td>2025年04月29日 06:26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9小時</td>\n",
              "      <td>I can’t wait to watch Beyoncé on TikTok tonigh...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>2656</td>\n",
              "      <td>29</td>\n",
              "      <td>280</td>\n",
              "      <td>...</td>\n",
              "      <td>2025年04月30日 00:24</td>\n",
              "      <td>🫶🏿🥹</td>\n",
              "      <td>3</td>\n",
              "      <td>en</td>\n",
              "      <td>2025-04-30 00:24:39.870994+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3099</th>\n",
              "      <td>other98</td>\n",
              "      <td>2025年04月29日 12:31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3小時</td>\n",
              "      <td>Tonight, Canada just proved that they have a h...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>10000</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>...</td>\n",
              "      <td>2025年04月30日 00:24</td>\n",
              "      <td>🙌</td>\n",
              "      <td>1</td>\n",
              "      <td>en</td>\n",
              "      <td>2025-04-30 00:24:54.669936+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3100</th>\n",
              "      <td>scottiebeam</td>\n",
              "      <td>2025年04月29日 01:33</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14小時</td>\n",
              "      <td>Yall be fighting on here … i thought threads w...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>427</td>\n",
              "      <td>76</td>\n",
              "      <td>28</td>\n",
              "      <td>...</td>\n",
              "      <td>2025年04月30日 00:25</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "      <td>2025-04-30 00:25:09.396585+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3101 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 author                 post_time topic time_info  \\\n",
              "0                ayofvr  2025-04-29T10:58:39.000Z   NaN       3小時   \n",
              "1                   NaN  2025-04-29T12:33:44.000Z   NaN       1小時   \n",
              "2        ribboworld2021  2025-04-29T04:39:46.000Z  小一日常       9小時   \n",
              "3                ayofvr  2025-04-29T11:25:22.000Z   NaN       3小時   \n",
              "4              jose_ykc  2025-04-29T10:36:19.000Z  輔仁大學       3小時   \n",
              "...                 ...                       ...   ...       ...   \n",
              "3096  leighton.williams         2025年04月29日 07:30   NaN       8小時   \n",
              "3097          cape__man         2025年04月29日 06:31   NaN       9小時   \n",
              "3098      simimoonlight         2025年04月29日 06:26   NaN       9小時   \n",
              "3099            other98         2025年04月29日 12:31   NaN       3小時   \n",
              "3100        scottiebeam         2025年04月29日 01:33   NaN      14小時   \n",
              "\n",
              "                                                content  has_photo  has_video  \\\n",
              "0                        Thank you God for another day.      False      False   \n",
              "1                                      百達翡麗？ 沒有下限的網路病態！       True      False   \n",
              "2              考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」      False      False   \n",
              "3     Just be strong. Confident. Hopeful. Intellectu...      False      False   \n",
              "4                             日文輔系老師上課內容之一AiScReam 歌詞導讀       True       True   \n",
              "...                                                 ...        ...        ...   \n",
              "3096                        First tasting in California      False      False   \n",
              "3097                 I hoped it would have been better.      False      False   \n",
              "3098  I can’t wait to watch Beyoncé on TikTok tonigh...      False      False   \n",
              "3099  Tonight, Canada just proved that they have a h...      False      False   \n",
              "3100  Yall be fighting on here … i thought threads w...      False      False   \n",
              "\n",
              "      like_count  reply_count  repost_count  ...        scrape_time  emojis  \\\n",
              "0            190            3            23  ...  2025年04月30日 06:27           \n",
              "1            196           16             0  ...  2025年04月30日 06:27           \n",
              "2             75            6             0  ...  2025年04月30日 06:28           \n",
              "3             83            5             3  ...  2025年04月30日 06:28           \n",
              "4           4334           55           513  ...  2025年04月30日 06:28           \n",
              "...          ...          ...           ...  ...                ...     ...   \n",
              "3096           7            0             0  ...  2025年04月30日 00:23           \n",
              "3097           2            0             0  ...  2025年04月30日 00:23           \n",
              "3098        2656           29           280  ...  2025年04月30日 00:24     🫶🏿🥹   \n",
              "3099       10000          214           214  ...  2025年04月30日 00:24       🙌   \n",
              "3100         427           76            28  ...  2025年04月30日 00:25           \n",
              "\n",
              "      emoji_count lang               scrape_time_origin post_weekday  \\\n",
              "0               0   en 2025-04-30 06:27:40.176749+08:00    Wednesday   \n",
              "1               0   Ch 2025-04-30 06:27:54.964788+08:00    Wednesday   \n",
              "2               0   Ch 2025-04-30 06:28:09.873641+08:00    Wednesday   \n",
              "3               0   en 2025-04-30 06:28:24.726576+08:00    Wednesday   \n",
              "4               0   Ch 2025-04-30 06:28:39.393706+08:00    Wednesday   \n",
              "...           ...  ...                              ...          ...   \n",
              "3096            0   en 2025-04-30 00:23:40.328046+08:00    Wednesday   \n",
              "3097            0   en 2025-04-30 00:23:55.063517+08:00    Wednesday   \n",
              "3098            3   en 2025-04-30 00:24:39.870994+08:00    Wednesday   \n",
              "3099            1   en 2025-04-30 00:24:54.669936+08:00    Wednesday   \n",
              "3100            0   en 2025-04-30 00:25:09.396585+08:00    Wednesday   \n",
              "\n",
              "      post_hour viral has_question has_exclaim  \n",
              "0             6     0        False       False  \n",
              "1             6     1         True        True  \n",
              "2             6     0        False        True  \n",
              "3             6     0        False       False  \n",
              "4             6     1        False       False  \n",
              "...         ...   ...          ...         ...  \n",
              "3096          0     0        False       False  \n",
              "3097          0     0        False       False  \n",
              "3098          0     1        False       False  \n",
              "3099          0     1        False        True  \n",
              "3100          0     1        False       False  \n",
              "\n",
              "[3101 rows x 24 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === 數值欄位清洗（萬字、逗號格式處理）===\n",
        "def parse_count(value):\n",
        "    # 將文字數字（如 \"1,234\"、\"2.5萬\"）統一轉為整數（int）\n",
        "    if pd.isna(value): return 0\n",
        "    value = str(value).replace(\",\", \"\")\n",
        "    # \"萬\" 的部分會乘上 10,000 做轉換\n",
        "    # 無法處理的格式就回傳 0\n",
        "    if \"萬\" in value:\n",
        "        return int(float(value.replace(\"萬\", \"\")) * 10000)\n",
        "    try:\n",
        "        return int(float(value))\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "for col in [\"like_count\", \"view_count\", \"share_count\", \"repost_count\", \"reply_count\"]:\n",
        "    df[col] = df[col].apply(parse_count)\n",
        "\n",
        "# === 布林欄位處理 ===\n",
        "# 將原始欄位（Y/N）轉換為 True/False\n",
        "# 處理過程會去除空白、轉成大寫\n",
        "df[\"has_photo\"] = df[\"has_photo\"].apply(lambda x: str(x).strip().upper() == \"Y\")\n",
        "df[\"has_video\"] = df[\"has_video\"].apply(lambda x: str(x).strip().upper() == \"Y\")\n",
        "\n",
        "# === emoji 萃取與統計 ===\n",
        "# 檢查是否為文字型別，如果是文字，從中萃取出所有 emoji 字元並串接成字串回傳\n",
        "def extract_emojis(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    return \"\".join([ch for ch in text if ch in emoji.EMOJI_DATA])\n",
        "\n",
        "df[\"emojis\"] = df[\"content\"].apply(extract_emojis)\n",
        "df[\"emoji_count\"] = df[\"emojis\"].apply(len)\n",
        "\n",
        "df[\"lang\"] = df[\"content\"].apply(detect_lang_with_preprocessing_lingua)\n",
        "\n",
        "# === scrape_time 處理（轉換時區 + 抽取星期與小時）===\n",
        "# 將時間欄位轉為台北時區，額外抽出格式化後的時間字串、星期幾、小時(0–23）\n",
        "df[\"scrape_time_origin\"] = pd.to_datetime(df[\"scrape_time\"], utc=True).dt.tz_convert(\"Asia/Taipei\")\n",
        "df[\"scrape_time\"]  = df[\"scrape_time_origin\"].dt.strftime(\"%Y年%m月%d日 %H:%M\")\n",
        "df[\"post_weekday\"] = df[\"scrape_time_origin\"].dt.day_name()\n",
        "df[\"post_hour\"] = df[\"scrape_time_origin\"].dt.hour\n",
        "\n",
        "# === 是否為高流量文章（破萬）===\n",
        "# 超過等於 10,000 瀏覽為 1，其餘為 0\n",
        "df[\"viral\"] = (df[\"view_count\"] >= 10000).astype(int)\n",
        "\n",
        "# === 是否使用問號、驚嘆號 ===\n",
        "df[\"has_question\"] = df[\"content\"].apply(lambda x: \"？\" in str(x) or \"?\" in str(x))\n",
        "df[\"has_exclaim\"] = df[\"content\"].apply(lambda x: \"！\" in str(x) or \"!\" in str(x))\n",
        "\n",
        "# === 儲存結果 ===\n",
        "df.to_csv(\"threads_cleaned_v1.csv\",encoding='utf_8_sig', index=False)\n",
        "print(\"✅ 處理完成，已輸出 threads_cleaned_v1.csv\")\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.2.2\n",
            "Uninstalling torch-2.2.2:\n",
            "  Successfully uninstalled torch-2.2.2\n",
            "Found existing installation: torchvision 0.17.2\n",
            "Uninstalling torchvision-0.17.2:\n",
            "  Successfully uninstalled torchvision-0.17.2\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torch torchvision -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch==2.2.2\n",
            "  Using cached torch-2.2.2-cp310-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
            "Collecting torchvision==0.17.2\n",
            "  Downloading torchvision-0.17.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.2.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.2.2) (4.13.2)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.2.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from torch==2.2.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.2.2) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision==0.17.2) (2.2.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision==0.17.2) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from jinja2->torch==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Downloading torch-2.2.2-cp310-none-macosx_11_0_arm64.whl (59.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m474.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.17.2-cp310-cp310-macosx_11_0_arm64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m635.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.2\n",
            "    Uninstalling torch-2.1.2:\n",
            "      Successfully uninstalled torch-2.1.2\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.22.0\n",
            "    Uninstalling torchvision-0.22.0:\n",
            "      Successfully uninstalled torchvision-0.22.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.7.0 requires torch==2.7.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.2.2 torchvision-0.17.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torch==2.2.2 torchvision==0.17.2 --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)\n",
            "Downloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m990.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.6\n",
            "    Uninstalling numpy-2.2.6:\n",
            "      Successfully uninstalled numpy-2.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-macos 2.13.1 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
            "tensorflow-macos 2.13.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.13.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install numpy==1.26.4 --force-reinstall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a29f20e1e1d4900b00d4276c6948906",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ddd88944b3f4a12a81f107600b5481b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6a444527cdc4464a405084fe7a9e551",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a96b8fcbacd64299a5afcfb618c629df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "309a792ffe5941ef841babdb695f2dad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7590f167bb1741c79dd119e7a9675490",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b60ede867704f19afc8e76b3b6334cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/409M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error while downloading from https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/refs%2Fpr%2F19/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
            "Trying to resume download...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74d8c423bb03484498452256c86519f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:  23%|##3       | 115M/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error while downloading from https://cdn-lfs.hf.co/uer/roberta-base-finetuned-jd-binary-chinese/94480df24580c4e614f9cd1a738d8aa100746b7b80051258c86132a0eecd1531?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1748246704&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0ODI0NjcwNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby91ZXIvcm9iZXJ0YS1iYXNlLWZpbmV0dW5lZC1qZC1iaW5hcnktY2hpbmVzZS85NDQ4MGRmMjQ1ODBjNGU2MTRmOWNkMWE3MzhkOGFhMTAwNzQ2YjdiODAwNTEyNThjODYxMzJhMGVlY2QxNTMxP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=I6I0-Bh6XBQmr9tdGwHRN-a99r-xsQYMmrVRix60ZvgNKt-jXU1XnAC19UsrrjKPl%7EqnubtHmbK6tnCcOnhIIWN2YgOBO56Pe2pvXCvndE1cZINSrs8o%7EWvz7%7EGN6v8XLJesilcrS%7Ea4cM0uB-tm1dHIXcKF70Um3fDJVn8gN5gr3nRCGScYI8Zg4bjvC0WgJTh5nqAtjly6rmEk4nmfVkVXkBYC%7EZqZtFWeE--WODJEvCynfqBntdKt-iszfuslLeeYOYYp4bYYxIGyDTTuyUapmf3gEXpWZHBfkFZbfNvH3Am9IsASi0NOl3KnF%7E8ingQW8KcGe9COG0jgSmSbiA__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
            "Trying to resume download...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93ee43247f7c457dbd4e03ec536becf1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:  26%|##5       | 105M/409M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error while downloading from https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/refs%2Fpr%2F19/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
            "Trying to resume download...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ebf6cc9d7344ad1859a1b703ffd8b66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:  32%|###1      | 157M/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error while downloading from https://cdn-lfs.hf.co/uer/roberta-base-finetuned-jd-binary-chinese/94480df24580c4e614f9cd1a738d8aa100746b7b80051258c86132a0eecd1531?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1748246704&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0ODI0NjcwNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby91ZXIvcm9iZXJ0YS1iYXNlLWZpbmV0dW5lZC1qZC1iaW5hcnktY2hpbmVzZS85NDQ4MGRmMjQ1ODBjNGU2MTRmOWNkMWE3MzhkOGFhMTAwNzQ2YjdiODAwNTEyNThjODYxMzJhMGVlY2QxNTMxP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=I6I0-Bh6XBQmr9tdGwHRN-a99r-xsQYMmrVRix60ZvgNKt-jXU1XnAC19UsrrjKPl%7EqnubtHmbK6tnCcOnhIIWN2YgOBO56Pe2pvXCvndE1cZINSrs8o%7EWvz7%7EGN6v8XLJesilcrS%7Ea4cM0uB-tm1dHIXcKF70Um3fDJVn8gN5gr3nRCGScYI8Zg4bjvC0WgJTh5nqAtjly6rmEk4nmfVkVXkBYC%7EZqZtFWeE--WODJEvCynfqBntdKt-iszfuslLeeYOYYp4bYYxIGyDTTuyUapmf3gEXpWZHBfkFZbfNvH3Am9IsASi0NOl3KnF%7E8ingQW8KcGe9COG0jgSmSbiA__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
            "Trying to resume download...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c17c9d49dfb9427d800e4953e7435992",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:  46%|####6     | 189M/409M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error while downloading from https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/refs%2Fpr%2F19/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
            "Trying to resume download...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40a96f8af9a74c059ec9a1e53f4e6cf7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/295 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "310c239b87cb40758b2202a21567d080",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/409M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93bc13a445784562a6bec0f5d7621fe5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 英文社群文本\n",
        "en_classifier = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "\n",
        "# 中文文本（可替換為你想要的模型）\n",
        "zh_classifier = pipeline(\"sentiment-analysis\", model=\"uer/roberta-base-finetuned-jd-binary-chinese\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_transformer_sentiment(text, lang):\n",
        "    try:\n",
        "        if lang == \"en\":\n",
        "            result = en_classifier(text)[0]\n",
        "        elif lang == \"Ch\":\n",
        "            result = zh_classifier(text)[0]\n",
        "        else:\n",
        "            return \"neutral\", 0.5  # fallback\n",
        "\n",
        "        label = result['label'].lower()\n",
        "        raw_score = result['score']  # 使用模型的信心分數\n",
        "\n",
        "        mapped_label = \"positive\" if \"pos\" in label else \"negative\" if \"neg\" in label else \"neutral\"\n",
        "        return mapped_label, raw_score\n",
        "    except:\n",
        "        return \"neutral\", 0.5\n",
        "    \n",
        "\n",
        "\n",
        "def adjust_sentiment_by_emoji(text, base_score):\n",
        "    if \"😂\" in text or \"😍\" in text:\n",
        "        return min(1.0, base_score + 0.1)\n",
        "    elif \"😡\" in text or \"💀\" in text:\n",
        "        return max(0.0, base_score - 0.2)\n",
        "    elif text.count(\"!\") > 2 or \"超\" in text:\n",
        "        return min(1.0, base_score + 0.05)\n",
        "    return base_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sentiment_score_and_label(text, lang):\n",
        "    if not isinstance(text, str):\n",
        "        return 0.5, \"neutral\"\n",
        "    \n",
        "    label, raw_score = classify_transformer_sentiment(text, lang)\n",
        "\n",
        "    # 對原始 score 做簡單轉換：\n",
        "    if label == \"positive\":\n",
        "        score = 0.5 + (raw_score / 2)  # ex: 0.98 → ~0.99\n",
        "    elif label == \"negative\":\n",
        "        score = 0.5 - (raw_score / 2)  # ex: 0.97 → ~0.01\n",
        "    else:\n",
        "        score = 0.5\n",
        "\n",
        "    adjusted_score = adjust_sentiment_by_emoji(text, score)\n",
        "\n",
        "    final_label = (\n",
        "        \"positive\" if adjusted_score > 0.66 else\n",
        "        \"negative\" if adjusted_score < 0.33 else\n",
        "        \"neutral\"\n",
        "    )\n",
        "\n",
        "    return adjusted_score, final_label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 全部處理完成，已輸出 threads_sentiment_label.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>post_time</th>\n",
              "      <th>topic</th>\n",
              "      <th>time_info</th>\n",
              "      <th>content</th>\n",
              "      <th>has_photo</th>\n",
              "      <th>has_video</th>\n",
              "      <th>like_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>repost_count</th>\n",
              "      <th>...</th>\n",
              "      <th>emoji_count</th>\n",
              "      <th>lang</th>\n",
              "      <th>scrape_time_origin</th>\n",
              "      <th>post_weekday</th>\n",
              "      <th>post_hour</th>\n",
              "      <th>viral</th>\n",
              "      <th>has_question</th>\n",
              "      <th>has_exclaim</th>\n",
              "      <th>sentiment_score</th>\n",
              "      <th>sentiment_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ayofvr</td>\n",
              "      <td>2025-04-29T10:58:39.000Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3小時</td>\n",
              "      <td>Thank you God for another day.</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>190</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "      <td>2025-04-30 06:27:40.176749+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2025-04-29T12:33:44.000Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1小時</td>\n",
              "      <td>百達翡麗？ 沒有下限的網路病態！</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>196</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>Ch</td>\n",
              "      <td>2025-04-30 06:27:54.964788+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>0.100839</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ribboworld2021</td>\n",
              "      <td>2025-04-29T04:39:46.000Z</td>\n",
              "      <td>小一日常</td>\n",
              "      <td>9小時</td>\n",
              "      <td>考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>75</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>Ch</td>\n",
              "      <td>2025-04-30 06:28:09.873641+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.954651</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ayofvr</td>\n",
              "      <td>2025-04-29T11:25:22.000Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3小時</td>\n",
              "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>83</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "      <td>2025-04-30 06:28:24.726576+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jose_ykc</td>\n",
              "      <td>2025-04-29T10:36:19.000Z</td>\n",
              "      <td>輔仁大學</td>\n",
              "      <td>3小時</td>\n",
              "      <td>日文輔系老師上課內容之一AiScReam 歌詞導讀</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>4334</td>\n",
              "      <td>55</td>\n",
              "      <td>513</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>Ch</td>\n",
              "      <td>2025-04-30 06:28:39.393706+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.989272</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3096</th>\n",
              "      <td>leighton.williams</td>\n",
              "      <td>2025年04月29日 07:30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8小時</td>\n",
              "      <td>First tasting in California</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "      <td>2025-04-30 00:23:40.328046+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3097</th>\n",
              "      <td>cape__man</td>\n",
              "      <td>2025年04月29日 06:31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9小時</td>\n",
              "      <td>I hoped it would have been better.</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "      <td>2025-04-30 00:23:55.063517+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3098</th>\n",
              "      <td>simimoonlight</td>\n",
              "      <td>2025年04月29日 06:26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9小時</td>\n",
              "      <td>I can’t wait to watch Beyoncé on TikTok tonigh...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>2656</td>\n",
              "      <td>29</td>\n",
              "      <td>280</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>en</td>\n",
              "      <td>2025-04-30 00:24:39.870994+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3099</th>\n",
              "      <td>other98</td>\n",
              "      <td>2025年04月29日 12:31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3小時</td>\n",
              "      <td>Tonight, Canada just proved that they have a h...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>10000</td>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>en</td>\n",
              "      <td>2025-04-30 00:24:54.669936+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3100</th>\n",
              "      <td>scottiebeam</td>\n",
              "      <td>2025年04月29日 01:33</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14小時</td>\n",
              "      <td>Yall be fighting on here … i thought threads w...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>427</td>\n",
              "      <td>76</td>\n",
              "      <td>28</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "      <td>2025-04-30 00:25:09.396585+08:00</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3101 rows × 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 author                 post_time topic time_info  \\\n",
              "0                ayofvr  2025-04-29T10:58:39.000Z   NaN       3小時   \n",
              "1                   NaN  2025-04-29T12:33:44.000Z   NaN       1小時   \n",
              "2        ribboworld2021  2025-04-29T04:39:46.000Z  小一日常       9小時   \n",
              "3                ayofvr  2025-04-29T11:25:22.000Z   NaN       3小時   \n",
              "4              jose_ykc  2025-04-29T10:36:19.000Z  輔仁大學       3小時   \n",
              "...                 ...                       ...   ...       ...   \n",
              "3096  leighton.williams         2025年04月29日 07:30   NaN       8小時   \n",
              "3097          cape__man         2025年04月29日 06:31   NaN       9小時   \n",
              "3098      simimoonlight         2025年04月29日 06:26   NaN       9小時   \n",
              "3099            other98         2025年04月29日 12:31   NaN       3小時   \n",
              "3100        scottiebeam         2025年04月29日 01:33   NaN      14小時   \n",
              "\n",
              "                                                content  has_photo  has_video  \\\n",
              "0                        Thank you God for another day.      False      False   \n",
              "1                                      百達翡麗？ 沒有下限的網路病態！       True      False   \n",
              "2              考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」      False      False   \n",
              "3     Just be strong. Confident. Hopeful. Intellectu...      False      False   \n",
              "4                             日文輔系老師上課內容之一AiScReam 歌詞導讀       True       True   \n",
              "...                                                 ...        ...        ...   \n",
              "3096                        First tasting in California      False      False   \n",
              "3097                 I hoped it would have been better.      False      False   \n",
              "3098  I can’t wait to watch Beyoncé on TikTok tonigh...      False      False   \n",
              "3099  Tonight, Canada just proved that they have a h...      False      False   \n",
              "3100  Yall be fighting on here … i thought threads w...      False      False   \n",
              "\n",
              "      like_count  reply_count  repost_count  ...  emoji_count  lang  \\\n",
              "0            190            3            23  ...            0    en   \n",
              "1            196           16             0  ...            0    Ch   \n",
              "2             75            6             0  ...            0    Ch   \n",
              "3             83            5             3  ...            0    en   \n",
              "4           4334           55           513  ...            0    Ch   \n",
              "...          ...          ...           ...  ...          ...   ...   \n",
              "3096           7            0             0  ...            0    en   \n",
              "3097           2            0             0  ...            0    en   \n",
              "3098        2656           29           280  ...            3    en   \n",
              "3099       10000          214           214  ...            1    en   \n",
              "3100         427           76            28  ...            0    en   \n",
              "\n",
              "                   scrape_time_origin post_weekday post_hour viral  \\\n",
              "0    2025-04-30 06:27:40.176749+08:00    Wednesday         6     0   \n",
              "1    2025-04-30 06:27:54.964788+08:00    Wednesday         6     1   \n",
              "2    2025-04-30 06:28:09.873641+08:00    Wednesday         6     0   \n",
              "3    2025-04-30 06:28:24.726576+08:00    Wednesday         6     0   \n",
              "4    2025-04-30 06:28:39.393706+08:00    Wednesday         6     1   \n",
              "...                               ...          ...       ...   ...   \n",
              "3096 2025-04-30 00:23:40.328046+08:00    Wednesday         0     0   \n",
              "3097 2025-04-30 00:23:55.063517+08:00    Wednesday         0     0   \n",
              "3098 2025-04-30 00:24:39.870994+08:00    Wednesday         0     1   \n",
              "3099 2025-04-30 00:24:54.669936+08:00    Wednesday         0     1   \n",
              "3100 2025-04-30 00:25:09.396585+08:00    Wednesday         0     1   \n",
              "\n",
              "      has_question has_exclaim sentiment_score sentiment_label  \n",
              "0            False       False        0.500000         neutral  \n",
              "1             True        True        0.100839        negative  \n",
              "2            False        True        0.954651        positive  \n",
              "3            False       False        0.500000         neutral  \n",
              "4            False       False        0.989272        positive  \n",
              "...            ...         ...             ...             ...  \n",
              "3096         False       False        0.500000         neutral  \n",
              "3097         False       False        0.500000         neutral  \n",
              "3098         False       False        0.500000         neutral  \n",
              "3099         False        True        0.500000         neutral  \n",
              "3100         False       False        0.500000         neutral  \n",
              "\n",
              "[3101 rows x 26 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[[\"sentiment_score\", \"sentiment_label\"]] = df.apply(\n",
        "    lambda row: pd.Series(get_sentiment_score_and_label(row[\"content\"], row[\"lang\"])),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df.to_csv(\"threads_sentiment_label.csv\",encoding='utf_8_sig', index=False)\n",
        "print(\"✅ 全部處理完成，已輸出 threads_sentiment_label.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# SnowNLP TextBlob 情緒分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3nEaGQCnxobj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 全部處理完成，已輸出 threads_sentiment_label.csv\n"
          ]
        }
      ],
      "source": [
        "# # SnowNLP TextBlob 情緒分析\n",
        "# #情緒分析（越接近1越正面）\n",
        "# # df[\"sentiment_score\"] = df[\"content\"].apply(lambda x: SnowNLP(str(x)).sentiments if pd.notna(x) else 0.5)\n",
        "\n",
        "# from snownlp import SnowNLP\n",
        "# from textblob import TextBlob\n",
        "# import numpy as np\n",
        "# from opencc import OpenCC\n",
        "# cc = OpenCC('tw2sp')\n",
        "\n",
        "# def get_sentiment_score(text, lang):\n",
        "#     if not isinstance(text, str):\n",
        "#         return 0.5\n",
        "#     try:\n",
        "#         if lang == \"Ch\":\n",
        "#             text_simp = cc.convert(text)  # 先轉簡體\n",
        "#             return SnowNLP(text_simp).sentiments\n",
        "#         elif lang == \"en\":\n",
        "#             return (TextBlob(text).sentiment.polarity + 1) / 2\n",
        "#         else:\n",
        "#             return 0.5\n",
        "#     except Exception as e:\n",
        "#         logging.warning(f\"Sentiment error: {e} | text: {text}\")\n",
        "#         return 0.5\n",
        "\n",
        "# df[\"sentiment_score\"] = df.apply(lambda row: get_sentiment_score(row[\"content\"], row[\"lang\"]), axis=1)\n",
        "\n",
        "\n",
        "# def classify_sentiment(score):\n",
        "#     if score > 0.66:\n",
        "#         return \"positive\"\n",
        "#     elif score < 0.33:\n",
        "#         return \"negative\"\n",
        "#     else:\n",
        "#         return \"neutral\"\n",
        "# df[\"sentiment_label\"] = df[\"sentiment_score\"].apply(classify_sentiment)\n",
        "# df\n",
        "\n",
        "# df.to_csv(\"threads_sentiment_label.csv\",encoding='utf_8_sig', index=False)\n",
        "# print(\"✅ 全部處理完成，已輸出 threads_sentiment_label.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import text2emotion as te\n",
        "df[['joy', 'anger', 'sadness', 'fear', 'surprise']] = df['content'].apply(lambda x: pd.Series(te.get_emotion(str(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 人稱視角分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "主詞: I\n",
            "主詞: you\n"
          ]
        }
      ],
      "source": [
        "# 一次性下載（只需要跑一次）\n",
        "import spacy\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "\n",
        "# 然後載入\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# 測試一下\n",
        "doc = nlp(\"I hope you can see me.\")\n",
        "for token in doc:\n",
        "    if token.dep_ == \"nsubj\":\n",
        "        print(\"主詞:\", token.text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hanlp[full] in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.1.1)\n",
            "Requirement already satisfied: hanlp-common>=0.0.23 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp[full]) (0.0.23)\n",
            "Requirement already satisfied: hanlp-downloader in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp[full]) (0.0.25)\n",
            "Requirement already satisfied: hanlp-trie>=0.0.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp[full]) (0.0.5)\n",
            "Requirement already satisfied: pynvml in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp[full]) (12.0.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp[full]) (0.2.0)\n",
            "Requirement already satisfied: termcolor in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp[full]) (3.1.0)\n",
            "Requirement already satisfied: toposort==1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp[full]) (1.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp[full]) (2.7.0)\n",
            "Requirement already satisfied: transformers>=4.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp[full]) (4.51.3)\n",
            "Collecting fasttext-wheel==0.9.2 (from hanlp[full])\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-macosx_12_0_arm64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp[full]) (3.4.2)\n",
            "Collecting penman==1.2.1 (from hanlp[full])\n",
            "  Downloading Penman-1.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting perin-parser>=0.0.12 (from hanlp[full])\n",
            "  Downloading perin_parser-0.0.19.tar.gz (232 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting tensorflow<2.14,>=2.6.0 (from hanlp[full])\n",
            "  Downloading tensorflow-2.13.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (2.6 kB)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel==0.9.2->hanlp[full])\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fasttext-wheel==0.9.2->hanlp[full]) (65.5.0)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fasttext-wheel==0.9.2->hanlp[full]) (2.2.4)\n",
            "Requirement already satisfied: phrasetree>=0.0.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hanlp-common>=0.0.23->hanlp[full]) (0.0.9)\n",
            "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from perin-parser>=0.0.12->hanlp[full]) (1.15.2)\n",
            "Collecting tensorflow-macos==2.13.1 (from tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading tensorflow_macos-2.13.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.2 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=23.1.21 (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting h5py>=2.9.0 (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading h5py-3.13.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Collecting numpy (from fasttext-wheel==0.9.2->hanlp[full])\n",
            "  Downloading numpy-1.24.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging in /Users/alan/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full]) (24.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading protobuf-4.25.7-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full]) (1.16.0)\n",
            "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full]) (1.17.2)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading grpcio-1.71.0-cp310-cp310-macosx_12_0_universal2.whl.metadata (3.8 kB)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.6.0->hanlp[full]) (3.18.0)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch>=1.6.0 (from hanlp[full])\n",
            "  Downloading torch-2.6.0-cp310-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
            "  Downloading torch-2.5.1-cp310-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
            "  Downloading torch-2.5.0-cp310-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
            "  Downloading torch-2.4.1-cp310-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
            "  Downloading torch-2.4.0-cp310-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
            "  Downloading torch-2.3.1-cp310-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
            "  Downloading torch-2.3.0-cp310-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
            "INFO: pip is still looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading torch-2.2.2-cp310-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
            "  Downloading torch-2.2.1-cp310-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
            "  Downloading torch-2.2.0-cp310-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
            "  Downloading torch-2.1.2-cp310-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.6.0->hanlp[full]) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from torch>=1.6.0->hanlp[full]) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.6.0->hanlp[full]) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=4.1.1->hanlp[full]) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from transformers>=4.1.1->hanlp[full]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=4.1.1->hanlp[full]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=4.1.1->hanlp[full]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=4.1.1->hanlp[full]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=4.1.1->hanlp[full]) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=4.1.1->hanlp[full]) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pynvml->hanlp[full]) (12.575.51)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from jinja2->torch>=1.6.0->hanlp[full]) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests->transformers>=4.1.1->hanlp[full]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests->transformers>=4.1.1->hanlp[full]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests->transformers>=4.1.1->hanlp[full]) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/alan/Library/Python/3.10/lib/python/site-packages (from requests->transformers>=4.1.1->hanlp[full]) (2024.2.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch>=1.6.0->hanlp[full]) (1.3.0)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading google_auth-2.40.2-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full]) (3.1.3)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow<2.14,>=2.6.0->hanlp[full])\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
            "Downloading fasttext_wheel-0.9.2-cp310-cp310-macosx_12_0_arm64.whl (324 kB)\n",
            "Downloading Penman-1.2.1-py3-none-any.whl (43 kB)\n",
            "Downloading tensorflow-2.13.1-cp310-cp310-macosx_12_0_arm64.whl (1.9 kB)\n",
            "Downloading tensorflow_macos-2.13.1-cp310-cp310-macosx_12_0_arm64.whl (189.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 MB\u001b[0m \u001b[31m428.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:13\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.1.2-cp310-none-macosx_11_0_arm64.whl (59.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m468.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.3-cp310-cp310-macosx_11_0_arm64.whl (13.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m523.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
            "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Downloading grpcio-1.71.0-cp310-cp310-macosx_12_0_universal2.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m477.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.13.0-cp310-cp310-macosx_11_0_arm64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m517.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m362.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m392.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Downloading protobuf-4.25.7-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
            "Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m279.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "Downloading google_auth-2.40.2-py2.py3-none-any.whl (216 kB)\n",
            "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading markdown-3.8-py3-none-any.whl (106 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Building wheels for collected packages: perin-parser\n",
            "  Building wheel for perin-parser (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for perin-parser: filename=perin_parser-0.0.19-py3-none-any.whl size=316837 sha256=215879a4d7d0085a6cb2422fa46b84a06bfc861698aa7ab50081ab3030938913\n",
            "  Stored in directory: /Users/alan/Library/Caches/pip/wheels/1c/82/b8/f85fe25ea42d814142b06745bdde1e5a490ba9cc3dce3a9964\n",
            "Successfully built perin-parser\n",
            "Installing collected packages: penman, libclang, flatbuffers, wheel, typing-extensions, tensorflow-estimator, tensorboard-data-server, pybind11, pyasn1, protobuf, opt-einsum, oauthlib, numpy, markdown, keras, grpcio, google-pasta, gast, cachetools, absl-py, torch, rsa, requests-oauthlib, pyasn1-modules, h5py, fasttext-wheel, astunparse, perin-parser, google-auth, google-auth-oauthlib, tensorboard, tensorflow-macos, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 6.31.0\n",
            "    Uninstalling protobuf-6.31.0:\n",
            "      Successfully uninstalled protobuf-6.31.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.7.0\n",
            "    Uninstalling torch-2.7.0:\n",
            "      Successfully uninstalled torch-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydantic 2.11.5 requires typing-extensions>=4.12.2, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.33.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "torchaudio 2.7.0 requires torch==2.7.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.22.0 requires torch==2.7.0, but you have torch 2.1.2 which is incompatible.\n",
            "typing-inspection 0.4.1 requires typing-extensions>=4.12.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed absl-py-2.2.2 astunparse-1.6.3 cachetools-5.5.2 fasttext-wheel-0.9.2 flatbuffers-25.2.10 gast-0.4.0 google-auth-2.40.2 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 keras-2.13.1 libclang-18.1.1 markdown-3.8 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.4.0 penman-1.2.1 perin-parser-0.0.19 protobuf-4.25.7 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybind11-2.13.6 requests-oauthlib-2.0.0 rsa-4.9.1 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-macos-2.13.1 torch-2.1.2 typing-extensions-4.5.0 wheel-0.45.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install \"hanlp[full]\" -U\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torchvision 0.15.2\n",
            "Uninstalling torchvision-0.15.2:\n",
            "  Successfully uninstalled torchvision-0.15.2\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.17.2-cp310-cp310-macosx_10_13_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m883.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: numpy in /Users/alan/anaconda3/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
            "Collecting torch==2.2.2\n",
            "  Downloading torch-2.2.2-cp310-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m492.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:09\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /Users/alan/anaconda3/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (1.11.1)\n",
            "Collecting typing-extensions>=4.8.0\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m972.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /Users/alan/anaconda3/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /Users/alan/anaconda3/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (3.1.2)\n",
            "Requirement already satisfied: networkx in /Users/alan/anaconda3/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (2.8.4)\n",
            "Requirement already satisfied: filelock in /Users/alan/anaconda3/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (3.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alan/anaconda3/lib/python3.10/site-packages (from jinja2->torch==2.2.2->torchvision) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/alan/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch==2.2.2->torchvision) (1.2.1)\n",
            "Installing collected packages: typing-extensions, torch, torchvision\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.4.0\n",
            "    Uninstalling typing_extensions-4.4.0:\n",
            "      Successfully uninstalled typing_extensions-4.4.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1\n",
            "    Uninstalling torch-2.0.1:\n",
            "      Successfully uninstalled torch-2.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2 requires torch==2.0.1, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.2.2 torchvision-0.17.2 typing-extensions-4.13.2\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torchvision -y\n",
        "!pip install torchvision --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to load https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20200109_022431.zip\n",
            "Please upgrade HanLP with:\n",
            "\n",
            "\tpip install --upgrade hanlp\n",
            "\n",
            "If the problem persists, please submit an issue to https://github.com/hankcs/HanLP/issues/new?labels=bug&template=bug_report.md\n",
            "When reporting an issue, make sure to paste the FULL ERROR LOG below.\n",
            "================================ERROR LOG BEGINS================================\n",
            "OS: macOS-15.3.2-arm64-arm-64bit\n",
            "Python: 3.10.10\n",
            "PyTorch: 2.1.2\n",
            "TensorFlow: 2.13.1\n",
            "HanLP: 2.1.1\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\npartially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)\n=================================ERROR LOG ENDS=================================",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhanlp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m hanlp_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mhanlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhanlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCTB7_BIAFFINE_DEP_ZH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m doc \u001b[38;5;241m=\u001b[39m hanlp_pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m我希望你能看到。\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtok/fine\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/hanlp/__init__.py:43\u001b[0m, in \u001b[0;36mload\u001b[0;34m(save_dir, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhanlp_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HANLP_VERBOSE\n\u001b[1;32m     42\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m HANLP_VERBOSE\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_from_meta_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeta.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/hanlp/utils/component_util.py:188\u001b[0m, in \u001b[0;36mload_from_meta_file\u001b[0;34m(save_dir, meta_filename, transform_only, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/hanlp/utils/component_util.py:99\u001b[0m, in \u001b[0;36mload_from_meta_file\u001b[0;34m(save_dir, meta_filename, transform_only, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt contain classpath field\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     obj: Component \u001b[38;5;241m=\u001b[39m \u001b[43mobject_from_classpath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m transform_only:\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;66;03m# noinspection PyUnresolvedReferences\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/hanlp_common/reflection.py:27\u001b[0m, in \u001b[0;36mobject_from_classpath\u001b[0;34m(classpath, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobject_from_classpath\u001b[39m(classpath, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 27\u001b[0m     classpath \u001b[38;5;241m=\u001b[39m \u001b[43mstr_to_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclasspath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misfunction(classpath):\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m classpath\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/hanlp_common/reflection.py:44\u001b[0m, in \u001b[0;36mstr_to_type\u001b[0;34m(classpath)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"convert class path in str format to a type\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m module_name, class_name \u001b[38;5;241m=\u001b[39m classpath\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m, class_name)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/hanlp/components/parsers/biaffine_parser_tf.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhanlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse_alg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unique_root, adjust_root_score, chu_liu_edmonds\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhanlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_transformer\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhanlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_component\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasComponent\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhanlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malg_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tarjan\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/hanlp/layers/transformers/loader_tf.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFAutoModel\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhanlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpt_imports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer_, AutoModel_\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_transformer\u001b[39m(transformer, max_seq_length, num_labels, tagging\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tokenizer_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     11\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer_\u001b[38;5;241m.\u001b[39mfrom_pretrained(transformer)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/hanlp/layers/transformers/pt_imports.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOKENIZERS_PARALLELISM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOKENIZERS_PARALLELISM\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertConfig, PretrainedConfig, AutoConfig, AutoTokenizer, PreTrainedTokenizer, \\\n\u001b[1;32m     12\u001b[0m     BertTokenizerFast, AlbertConfig, BertModel, AutoModel, PreTrainedModel, AutoModelForSequenceClassification, \\\n\u001b[1;32m     13\u001b[0m     AutoModelForTokenClassification, BartModel\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAutoModel_\u001b[39;00m(AutoModel):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/utils/import_utils.py:1956\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1955\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1956\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m   1958\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/utils/import_utils.py:1955\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1955\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1956\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/utils/import_utils.py:1969\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1967\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1968\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1970\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1971\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1972\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\npartially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)\n=================================ERROR LOG ENDS================================="
          ]
        }
      ],
      "source": [
        "# import hanlp\n",
        "# hanlp_pipeline = hanlp.load(hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH)\n",
        "# doc = hanlp_pipeline(\"我希望你能看到。\")\n",
        "# print(doc['tok/fine'])\n",
        "# print(doc['dep'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CI4sQEyWxvuJ"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'hanlp_pipeline' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m detect_person_view(text)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 套用\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson_view\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetect_person_view_auto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlang\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson_view\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mdescribe(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[24], line 47\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m detect_person_view(text)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 套用\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson_view\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mdetect_person_view_auto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlang\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson_view\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mdescribe(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "Cell \u001b[0;32mIn[24], line 40\u001b[0m, in \u001b[0;36mdetect_person_view_auto\u001b[0;34m(text, lang)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_person_view_auto\u001b[39m(text, lang):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCh\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdetect_person_view_advanced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m detect_person_view_en(text)\n",
            "Cell \u001b[0;32mIn[24], line 19\u001b[0m, in \u001b[0;36mdetect_person_view_advanced\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_person_view_advanced\u001b[39m(text):\n\u001b[0;32m---> 19\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mhanlp_pipeline\u001b[49m(text)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word, dep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtok/fine\u001b[39m\u001b[38;5;124m'\u001b[39m], doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdep\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dep \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSBV\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hanlp_pipeline' is not defined"
          ]
        }
      ],
      "source": [
        "# hanlp_pipeline = hanlp.load('LARGE_ALBERT_BASE')\n",
        "#人稱視角分析\n",
        "def detect_person_view(text):\n",
        "    if not isinstance(text, str): return \"none\"\n",
        "    text = text.lower()\n",
        "    first = any(p in text for p in [\"我\", \"我們\", \"i\", \"we\"])\n",
        "    second = any(p in text for p in [\"你\", \"你們\", \"you\"])\n",
        "    third = any(p in text for p in [\"他\", \"她\", \"他們\", \"她們\", \"he\", \"she\", \"they\", \"it\"])\n",
        "    flags = [first, second, third]\n",
        "    if flags.count(True) > 1:\n",
        "        return \"mixed\"\n",
        "    elif first: return \"1st\"\n",
        "    elif second: return \"2nd\"\n",
        "    elif third: return \"3rd\"\n",
        "    return \"none\"\n",
        "\n",
        "# 中文依賴分析\n",
        "def detect_person_view_advanced(text):\n",
        "    doc = hanlp_pipeline(text)\n",
        "    for word, dep in zip(doc['tok/fine'], doc['dep']):\n",
        "        if dep == 'SBV':\n",
        "            if word in ['我', '我們']: return '1st'\n",
        "            elif word in ['你', '你們']: return '2nd'\n",
        "            elif word in ['他', '她', '他們', '她們', '它', '它們']: return '3rd'\n",
        "    return 'none'\n",
        "\n",
        "# 英文依賴分析\n",
        "def detect_person_view_en(text):\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"nsubj\":\n",
        "            if token.text.lower() in [\"i\", \"we\"]: return \"1st\"\n",
        "            elif token.text.lower() == \"you\": return \"2nd\"\n",
        "            elif token.text.lower() in [\"he\", \"she\", \"they\", \"it\"]: return \"3rd\"\n",
        "    return \"none\"\n",
        "\n",
        "# 自動分流\n",
        "def detect_person_view_auto(text, lang):\n",
        "    if lang == \"Ch\":\n",
        "        return detect_person_view_advanced(text)\n",
        "    elif lang == \"en\":\n",
        "        return detect_person_view_en(text)\n",
        "    else:\n",
        "        return detect_person_view(text)\n",
        "\n",
        "# 套用\n",
        "df[\"person_view\"] = df.apply(lambda row: detect_person_view_auto(row[\"content\"], row[\"lang\"]), axis=1)\n",
        "print(df[[\"sentiment_score\", \"person_view\"]].describe(include=\"all\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTVtegcwEfb1"
      },
      "source": [
        "## 清洗數據V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a9vHZ0p9o2u"
      },
      "outputs": [],
      "source": [
        "# --- 文章長度 ---\n",
        "df[\"content_length\"] = df[\"content\"].apply(lambda x: len(str(x)))\n",
        "\n",
        "# --- 是否包含網址 ---\n",
        "df[\"has_url\"] = df[\"content\"].apply(lambda x: \"http\" in str(x) or \"www.\" in str(x))\n",
        "\n",
        "# --- 是否包含 @標記他人 ---\n",
        "df[\"has_mention\"] = df[\"content\"].apply(lambda x: \"@\" in str(x))\n",
        "\n",
        "# --- 是否使用 Hashtag ---\n",
        "df[\"has_hashtag\"] = df[\"content\"].apply(lambda x: \"#\" in str(x))\n",
        "\n",
        "# 貼文主題字詞提取（可後續做 TF-IDF 或主題建模）\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=100, stop_words='english')\n",
        "word_matrix = vectorizer.fit_transform(df['content'].astype(str))\n",
        "\n",
        "# 將常見詞語提取出來\n",
        "keywords = vectorizer.get_feature_names_out()\n",
        "\n",
        "# 是否為深夜或白天貼文（時間段分類）\n",
        "def time_period(hour):\n",
        "    if 5 <= hour < 12:\n",
        "        return \"morning\"\n",
        "    elif 12 <= hour < 17:\n",
        "        return \"afternoon\"\n",
        "    elif 17 <= hour < 22:\n",
        "        return \"evening\"\n",
        "    else:\n",
        "        return \"night\"\n",
        "\n",
        "# 判斷是否合法的time_info格式\n",
        "def is_valid_time_info(text):\n",
        "    if pd.isna(text):\n",
        "        return False\n",
        "    text = str(text).strip()\n",
        "    return bool(re.match(r\"^\\d+\\s*(分鐘|小時|天)$\", text))\n",
        "\n",
        "df = df[df[\"time_info\"].apply(is_valid_time_info)].copy()\n",
        "\n",
        "# 將time_info 統一轉為小時\n",
        "def convert_time_info(text):\n",
        "    if pd.isna(text):\n",
        "        return 0\n",
        "    text = str(text)\n",
        "    if \"分鐘\" in text:\n",
        "        match = re.search(r\"(\\d+)\", text)\n",
        "        return int(match.group(1)) / 60\n",
        "    elif \"小時\" in text:\n",
        "        match = re.search(r\"(\\d+)\", text)\n",
        "        return int(match.group(1))\n",
        "    elif \"天\" in text:\n",
        "        match = re.search(r\"(\\d+)\", text)\n",
        "        return int(match.group(1)) * 24\n",
        "    elif \"週\" in text or \"禮拜\" in text:\n",
        "        match = re.search(r\"(\\d+)\", text)\n",
        "        return int(match.group(1)) * 7 * 24\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df[\"time_elapsed_hours\"] = df[\"time_info\"].apply(convert_time_info)\n",
        "df[\"post_period\"] = df[\"post_hour\"].apply(time_period)\n",
        "\n",
        "cols_to_show_first = ['author', 'content', 'content_length', 'lang', 'scrape_time', 'post_weekday', 'post_hour', 'post_period', 'viral']\n",
        "df = df[cols_to_show_first + [col for col in df.columns if col not in cols_to_show_first]]\n",
        "df.to_csv(\"threads_cleaned_v2.csv\",encoding='utf_8_sig',index=False)\n",
        "print(\"✅ 處理完成，已輸出 threads_cleaned_v2.csv\")\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsJcFXGxuqYQ"
      },
      "source": [
        "## Self-Attention 模組處理文字資料（測試中 因未寫完可先跳過）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl7JYdlCuqYR"
      },
      "outputs": [],
      "source": [
        "# BERT 會把 [EMOJI], [CONTENT] 當作分界的詞來理解\n",
        "def build_semantic_text(row):\n",
        "    parts = []\n",
        "\n",
        "    if pd.notna(row[\"author\"]):\n",
        "        parts.append(f\"[AUTHOR] {row['author']}\")\n",
        "\n",
        "    if pd.notna(row[\"topic\"]):\n",
        "        parts.append(f\"[TOPIC] {row['topic']}\")\n",
        "\n",
        "    if pd.notna(row[\"emojis\"]):\n",
        "        emoji_text = \" \".join(row[\"emojis\"])\n",
        "        parts.append(f\"[EMOJI] {emoji_text}\")\n",
        "\n",
        "    if pd.notna(row[\"content\"]):\n",
        "        parts.append(f\"[CONTENT] {row['content']}\")\n",
        "\n",
        "    return \" \".join(parts)\n",
        "\n",
        "\n",
        "# 建立 semantic_text，不影響 df 本身\n",
        "semantic_text_series = df.apply(build_semantic_text, axis=1)\n",
        "\n",
        "# 若你想要新 DataFrame：\n",
        "df_semantic = df.copy()\n",
        "df_semantic[\"semantic_text\"] = semantic_text_series\n",
        "df_semantic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-a92qlduqYR"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 載入 BERT tokenizer 和模型\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
        "model = BertModel.from_pretrained(\"bert-base-chinese\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# 自訂 Dataset 類別（不變）\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, max_len=128):\n",
        "        self.encodings = tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
        "            \"attention_mask\": self.encodings[\"attention_mask\"][idx]\n",
        "        }\n",
        "\n",
        "# 建立 dataset 和 dataloader（這裡建議用 semantic_text 而不是 content）\n",
        "texts = df_semantic[\"semantic_text\"].astype(str).tolist()\n",
        "text_dataset = TextDataset(texts)\n",
        "text_loader = DataLoader(text_dataset, batch_size=32)\n",
        "\n",
        "# 提取語意向量 Z_text\n",
        "Z_text_list = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in text_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_rep = outputs.last_hidden_state[:, 0, :]  # 取 [CLS] 向量\n",
        "\n",
        "        Z_text_list.append(cls_rep.cpu())\n",
        "\n",
        "Z_text_tensor = torch.cat(Z_text_list, dim=0)  # [N, 768]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJqJ6xDJuqYS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "Z_text_df = pd.DataFrame(Z_text_tensor.numpy())\n",
        "Z_text_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-Hi6-1KuqYS"
      },
      "source": [
        "## MLP 處理數值資料"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QIUxXQHuqYS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 假設你已載入 DataFrame 為 df，並有這些欄位\n",
        "numeric_cols = [\n",
        "    \"view_count\", \"followers_count\", \"emoji_count\", \"content_length\", \"post_hour\", \"time_elapsed_hours\",\n",
        "    \"has_photo\", \"like_count\", \"reply_count\", \"repost_count\", \"share_count\",\n",
        "    \"has_hashtag\", \"has_url\", \"has_mention\", \"has_exclaim\", \"has_question\"\n",
        "]\n",
        "\n",
        "# 布林欄位轉 int（保險起見）\n",
        "for col in numeric_cols:\n",
        "    if df_semantic[col].dtype == bool:\n",
        "        df_semantic[col] = df_semantic[col].astype(int)\n",
        "\n",
        "# 標準化數值特徵\n",
        "scaler = StandardScaler()\n",
        "X_num_scaled = scaler.fit_transform(df_semantic[numeric_cols])\n",
        "\n",
        "# 定義簡單的 MLP 模型\n",
        "class NumericMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, output_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# 初始化與執行模型\n",
        "# 自動偵測你是否有 GPU（用 CUDA），否則就用 CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# 根據你數值欄位的數量設定 input_dim\n",
        "model_num = NumericMLP(input_dim=X_num_scaled.shape[1])\n",
        "# 把模型送到對應裝置（CPU / GPU）\n",
        "model_num.to(device)\n",
        "\n",
        "# 暫時關閉梯度運算，因沒有要訓練模型，僅讓資料過神經網路\n",
        "with torch.no_grad():\n",
        "    X_tensor = torch.tensor(X_num_scaled, dtype=torch.float32).to(device)\n",
        "    Z_num_tensor = model_num(X_tensor).cpu()\n",
        "\n",
        "# 合併 Z_text 和 Z_num\n",
        "Z_text_tensor = torch.tensor(Z_text_df.values, dtype=torch.float32)  # [N, 768]\n",
        "Z_full_tensor = torch.cat([Z_text_tensor, Z_num_tensor], dim=1)      # [N, 896]\n",
        "\n",
        "print(\"數值向量與語意向量已成功融合為 Z_full_tensor\")\n",
        "Z_full_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9GcFmuNEsVn"
      },
      "source": [
        "## 清洗數據embbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H23LR9aF0ny"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# =============== BERT 向量嵌入 ===============\n",
        "df = df.dropna(subset=['content']) #要先處理content空值才能embedding\n",
        "# --- 載入 tokenizer & model ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
        "# 選擇硬體設備（MPS、CUDA、CPU），自動判斷是否可用 GPU（M1/M2 晶片上的 MPS 或 CUDA），否則 fallback 到 CPU\n",
        "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# --- 建立 HuggingFace Dataset ---\n",
        "hf_dataset = Dataset.from_pandas(df[[\"content\"]])\n",
        "\n",
        "# --- tokenize function ---\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['content'], truncation=True, padding='max_length', max_length=128)\n",
        "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# --- 取得 [CLS] 向量 ---\n",
        "def extract_embeddings(batch):\n",
        "    inputs = {k: torch.tensor(v).to(model.device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
        "    return {\"embeddings\": embeddings}\n",
        "\n",
        "# --- 批次轉換為 embeddings ---\n",
        "batch_size = 64\n",
        "embeddings_dataset = tokenized_dataset.map(extract_embeddings, batched=True, batch_size=batch_size)\n",
        "\n",
        "# =============== 匯出最終結果 ===============\n",
        "# embeddings_dataset[\"embeddings\"] 是 list of 768-dim vectors\n",
        "embedding_df = pd.DataFrame(embeddings_dataset[\"embeddings\"])\n",
        "final_df = pd.concat([df.reset_index(drop=True), embedding_df], axis=1)\n",
        "\n",
        "# 儲存\n",
        "# final_df.to_csv(\"C:/Users/User/Desktop/louis/threads_with_embeddings.csv\",encoding='utf_8_sig', index=False)\n",
        "final_df.to_csv(\"threads_with_embeddings.csv\",encoding='utf_8_sig', index=False)\n",
        "print(\"✅ 全部處理完成，已輸出 threads_with_embeddings.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WxP6bcNEywY"
      },
      "source": [
        "## 分詞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SovtoK069ox_"
      },
      "outputs": [],
      "source": [
        "stopwords = set(['的', '了', '在', '是', '和', '也', '與', '有', '為', '等'])\n",
        "\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    words = jieba.cut(text)\n",
        "    words_filtered = [word for word in words if word not in stopwords]\n",
        "    return ' '.join(words_filtered)\n",
        "\n",
        "\n",
        "df['processed_content'] = df['content'].apply(tokenize_and_remove_stopwords)\n",
        "df['processed_content'][2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwCtvYh4E3vU"
      },
      "source": [
        "## 機器學習建模"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFdTaLb79ovp"
      },
      "outputs": [],
      "source": [
        "# 計算 TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_content'])\n",
        "\n",
        "# 計算 TF\n",
        "tf_vectorizer = CountVectorizer()\n",
        "tf_matrix = tf_vectorizer.fit_transform(df['processed_content'])\n",
        "\n",
        "print(tfidf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wvN8ioFE8Cz"
      },
      "source": [
        "# 多模型分類實驗"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cROM4E4v9otD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, random_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 8\n",
        "num_epochs = 5\n",
        "# ========== 參數設定 ==========\n",
        "model_tokenizer_map = {\n",
        "    \"FusionMacBERT\": \"hfl/chinese-macbert-base\",\n",
        "    \"PureMacBERT\": \"hfl/chinese-macbert-base\",\n",
        "    \"NumericOnly\": None,\n",
        "    \"BiLSTMWithNumeric\": \"bert-base-chinese\",\n",
        "    \"MacBERTWithGRU\": \"hfl/chinese-macbert-base\",\n",
        "    \"MacBERTMLPFusion\": \"hfl/chinese-macbert-base\",\n",
        "    \"TextCNNMacBERT\": \"hfl/chinese-macbert-base\",\n",
        "    \"RoBERTa\": \"hfl/chinese-roberta-wwm-ext\",\n",
        "    \"BERTwwmExt\": \"hfl/chinese-bert-wwm-ext\",\n",
        "    \"ERNIE\": \"nghuyong/ernie-3.0-base-zh\",\n",
        "    \"ConvBERT\": \"YituTech/conv-bert-base\"\n",
        "}\n",
        "\n",
        "#tokenizer\n",
        "default_tokenizer_name = model_tokenizer_map[\"FusionMacBERT\"]\n",
        "tokenizer = AutoTokenizer.from_pretrained(default_tokenizer_name)\n",
        "\n",
        "#載入資料\n",
        "# df = pd.read_csv(\"C:/Users/User/Desktop/louis/threads_cleaned_v2.csv\", encoding='utf_8_sig')\n",
        "#df = df.dropna(subset=['content', 'view_count']).reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To77Vfki2Jzf"
      },
      "source": [
        "## Label 分群 （1000以下、1000~10000、10000~100000、100000以上)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Df9cfTOt9oqs"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Label 分群 ：標籤轉換（按瀏覽數進行分群）\n",
        "# 取「瀏覽數」的第 80 百分位作為高人氣門檻（q_high）、第 20 百分位作為低人氣門檻（q_low）\n",
        "# 把每筆資料的「view_count」劃分為三類：0 高人氣 (high)、1 中人氣 (medium)、2 低人氣 (low)\n",
        "q_high = df['view_count'].quantile(0.80)\n",
        "q_low = df['view_count'].quantile(0.20)\n",
        "df['view_class'] = df['view_count'].apply(lambda x: \"high\" if x >= q_high else (\"low\" if x <= q_low else \"medium\"))\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['view_class'])\n",
        "\n",
        "#做 oversampling 類別資料平衡 ：資料增強（針對 high / low 類別 oversample）\n",
        "df_high = df[df['view_class'] == 'high']\n",
        "df_low = df[df['view_class'] == 'low']\n",
        "df_medium = df[df['view_class'] == 'medium']\n",
        "\n",
        "# 分別取出三個分類的樣本：對 high 與 low 分類做「過採樣」，各自複製三次，讓資料數量接近 medium\n",
        "# 再對整個資料表做隨機打散 (shuffle），避免模型學到資料順序的偏誤\n",
        "df_high_oversampled = pd.concat([df_high] * 3, ignore_index=True)\n",
        "df_low_oversampled = pd.concat([df_low] * 3, ignore_index=True)\n",
        "df = pd.concat([df_medium, df_high_oversampled, df_low_oversampled], ignore_index=True)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "'''\n",
        "\n",
        "\n",
        "def map_view_class(x):\n",
        "    if x < 1000:\n",
        "        return 'low'\n",
        "    elif x < 10000:\n",
        "        return 'medium'\n",
        "    elif x < 100000:\n",
        "        return 'high'\n",
        "    else:\n",
        "        return 'very_high'\n",
        "\n",
        "df['view_class'] = df['view_count'].apply(map_view_class)\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['view_class'])\n",
        "df_high = df[df['view_class'] == 'very_high']\n",
        "df_medium = df[df['view_class'] == 'high']\n",
        "df_low = df[df['view_class'] == 'medium']\n",
        "df_very_low = df[df['view_class'] == 'low']\n",
        "\n",
        "df_high_oversampled = pd.concat([df_high] * 3, ignore_index=True)\n",
        "df_very_low_oversampled = pd.concat([df_very_low] * 3, ignore_index=True)\n",
        "\n",
        "df = pd.concat([df_medium, df_low, df_high_oversampled, df_very_low_oversampled], ignore_index=True)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OLS 多元線性回歸分析\n",
        "看回歸係數、p 值、可視化，分析各特徵對預測的全域影響"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "906564ed"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#加入情緒與人稱特徵欄位\n",
        "df[\"person_view_encoded\"] = LabelEncoder().fit_transform(df[\"person_view\"])\n",
        "if \"sentiment_score\" not in df.columns:\n",
        "    df[\"sentiment_score\"] = 0.5\n",
        "#更新數值特徵欄位 ===\n",
        "num_cols = [\"like_count\", \"reply_count\", \"emoji_count\", \"sentiment_score\", \"person_view_encoded\"]\n",
        "\n",
        "#第二點(改用OLS)\n",
        "#轉為數值0跟1\n",
        "bool_cols = ['has_photo', 'has_video', 'has_question', 'has_exclaim', 'has_url', 'has_mention', 'has_hashtag']\n",
        "df[bool_cols] = df[bool_cols].astype(int)\n",
        "\n",
        "\n",
        "#OLS\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "X = df[[\n",
        "    'like_count', 'reply_count', 'share_count', 'repost_count',\n",
        "    'emoji_count', 'has_photo', 'has_video', 'content_length',\n",
        "    'has_question', 'has_exclaim', 'has_url', 'has_mention',\n",
        "    'has_hashtag', 'sentiment_score', 'person_view_encoded'\n",
        "]]\n",
        "y = df['view_count']\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "sns.regplot(x=\"like_count\", y=\"view_count\", data=df)\n",
        "plt.title(\"like_count對view_count的影響\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j32CLLcICUzu"
      },
      "outputs": [],
      "source": [
        "class RegressionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.input_ids = df['input_ids'].tolist()\n",
        "        self.attention_mask = df['attention_mask'].tolist()\n",
        "        self.numerics = df[num_cols].values\n",
        "        self.targets = df['target'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
        "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),\n",
        "            'target': torch.tensor(self.targets[idx], dtype=torch.float)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeqEslof2OBB"
      },
      "source": [
        "# Normalization 數值特徵標準化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3pJMu-g9on2"
      },
      "outputs": [],
      "source": [
        "# Normalization 數值特徵標準化\n",
        "base_num_cols = ['like_count', 'share_count', 'repost_count', 'reply_count', 'emoji_count', 'has_photo', 'has_video', 'has_question', 'has_exclaim', 'has_mention', 'has_url', 'has_hashtag', 'content_length']\n",
        "# 找出 one-hot 編碼的欄位（語言類型、發文時段、星期幾等類別欄位）\n",
        "# 使用 StandardScaler 將數值欄位轉換為「標準常態分布」（mean=0, std=1），有助於模型學習穩定。\n",
        "onehot_cols = [col for col in df.columns if col.startswith('lang_') or col.startswith('post_period_') or col.startswith('post_weekday_')]\n",
        "num_cols = base_num_cols + onehot_cols\n",
        "scaler = StandardScaler()\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# Tokenizer 文本編碼 ：使用事先定義好的 tokenizer（例如 MacBERT、RoBERTa）對貼文進行斷詞、編碼\n",
        "# 將編碼後的結果儲存到 df 中，這兩個欄位會作為 BERT 模型的輸入\n",
        "encodings = tokenizer(df['content'].tolist(), truncation=True, padding='max_length', max_length=128)\n",
        "df['input_ids'] = encodings['input_ids']  # 斷詞後對應的詞彙 ID\n",
        "df['attention_mask'] = encodings['attention_mask']  # 對應位置是否是 padding（0）或實際內容（1）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsA6q_qf9ojJ"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OBPv79a10W0"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.input_ids = df['input_ids'].tolist()\n",
        "        self.attention_mask = df['attention_mask'].tolist()\n",
        "        self.labels = df['label'].tolist()\n",
        "        self.numerics = df[num_cols].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
        "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        logp = self.ce(input, target)\n",
        "        p = torch.exp(-logp)\n",
        "        loss = (1 - p) ** self.gamma * logp\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_iVSfG82Gpt"
      },
      "source": [
        "# 模型架構"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um-YKfVt10Uf"
      },
      "outputs": [],
      "source": [
        "#模型架構\n",
        "# 1. FusionMacBERT：BERT + 數值特徵 concat\n",
        "class FusionMacBERTModel(nn.Module):\n",
        "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size + 64, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, numerics):\n",
        "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
        "        num_out = torch.relu(self.numeric_fc(numerics))\n",
        "        combined = torch.cat((cls_output, num_out), dim=1)\n",
        "        return self.classifier(self.dropout(combined))\n",
        "\n",
        "# 2. PureMacBERT：只有文字\n",
        "class PureMacBERTModel(nn.Module):\n",
        "    def __init__(self, model_name, num_classes):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, numerics=None):\n",
        "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
        "        return self.classifier(self.dropout(cls_output))\n",
        "\n",
        "# 3. NumericOnly：只有數值特徵\n",
        "class NumericOnlyModel(nn.Module):\n",
        "    def __init__(self, num_numeric_features, num_classes):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(num_numeric_features, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, numerics=None):\n",
        "        return self.classifier(numerics)\n",
        "\n",
        "# 4. BiLSTMWithNumeric：LSTM 處理詞嵌入 + 數值特徵\n",
        "class BiLSTMWithNumeric(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_numeric_features, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(hidden_dim * 2 + 64, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, numerics):\n",
        "        x = self.embedding(input_ids)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        pooled = lstm_out[:, -1, :]\n",
        "        num_out = torch.relu(self.numeric_fc(numerics))\n",
        "        combined = torch.cat((pooled, num_out), dim=1)\n",
        "        return self.classifier(self.dropout(combined))\n",
        "\n",
        "# 5. MacBERTWithGRU：BERT + GRU + 數值特徵\n",
        "class MacBERTWithGRU(nn.Module):\n",
        "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.gru = nn.GRU(self.bert.config.hidden_size, 128, batch_first=True, bidirectional=True)\n",
        "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
        "        self.classifier = nn.Linear(128*2 + 64, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, numerics):\n",
        "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "        gru_out, _ = self.gru(bert_out)\n",
        "        pooled = gru_out[:, -1, :]\n",
        "        num_out = torch.relu(self.numeric_fc(numerics))\n",
        "        combined = torch.cat((pooled, num_out), dim=1)\n",
        "        return self.classifier(self.dropout(combined))\n",
        "\n",
        "# 6. MacBERTMLPFusion：BERT + 數值特徵 -> MLP\n",
        "class MacBERTMLPFusion(nn.Module):\n",
        "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.bert.config.hidden_size + num_numeric_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, numerics):\n",
        "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
        "        combined = torch.cat((cls_output, numerics), dim=1)\n",
        "        return self.fc(combined)\n",
        "\n",
        "# 7. TextCNNMacBERT：BERT 輸出卷積後 + 數值特徵\n",
        "class TextCNNMacBERT(nn.Module):\n",
        "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, 64, (k, self.bert.config.hidden_size)) for k in [2, 3, 4]])\n",
        "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
        "        self.classifier = nn.Linear(64 * len([2, 3, 4]) + 64, num_classes)\n",
        "\n",
        "    def conv_and_pool(self, x, conv):\n",
        "        x = torch.relu(conv(x)).squeeze(3)\n",
        "        x = torch.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, numerics):\n",
        "        x = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state.unsqueeze(1)\n",
        "        cnn_out = torch.cat([self.conv_and_pool(x, conv) for conv in self.convs], 1)\n",
        "        num_out = torch.relu(self.numeric_fc(numerics))\n",
        "        combined = torch.cat((cnn_out, num_out), dim=1)\n",
        "        return self.classifier(combined)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De3g_-Qs2Vux"
      },
      "outputs": [],
      "source": [
        "#訓練與評估\n",
        "def train_and_eval(model, name, preview_count=10):\n",
        "    model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "    loss_fn = FocalLoss()\n",
        "    # loss_fn = nn.MSELoss()\n",
        "\n",
        "    # 訓練階段\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            numerics = batch['numerics'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            output = model(input_ids, attention_mask, numerics)\n",
        "            loss = loss_fn(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # 評估階段\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    '''\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            numerics = batch['numerics'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            output = model(input_ids, attention_mask, numerics)\n",
        "            preds = output.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "    mse = mean_squared_error(all_targets, all_preds)\n",
        "    mae = mean_absolute_error(all_targets, all_preds)\n",
        "    print(f\"[{name} 評估結果] MSE: {mse:.2f} | MAE: {mae:.2f}\")\n",
        "    print(f\"\\n{name} 評估結果：\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
        "    '''\n",
        "    preview_shown = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            numerics = batch['numerics'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            output = model(input_ids, attention_mask, numerics)\n",
        "            preds = output.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            #印出前幾筆的預測、真實值\n",
        "            if preview_shown < preview_count:\n",
        "                batch_size = input_ids.shape[0]\n",
        "                for i in range(batch_size):\n",
        "                    if preview_shown >= preview_count:\n",
        "                        break\n",
        "                    input_id = input_ids[i].cpu().numpy()\n",
        "                    text = tokenizer.decode(input_id, skip_special_tokens=True)\n",
        "                    print(f\"\\n[{name} 預測] 第 {preview_shown+1} 筆\")\n",
        "                    print(f\"Text: {text}\")\n",
        "                    print(f\"Predicted: {label_encoder.inverse_transform([preds[i]])[0]}\")\n",
        "                    print(f\"Actual:    {label_encoder.inverse_transform([labels[i].cpu().item()])[0]}\")\n",
        "                    preview_shown += 1\n",
        "\n",
        "    print(f\"\\n{name} 評估結果：\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t14MW4pL9mqz"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, random_split\n",
        "\n",
        "#轉換label\n",
        "df = df.dropna(subset=[\"content\", \"label\"]).reset_index(drop=True)\n",
        "df[\"label\"] = LabelEncoder().fit_transform(df[\"label\"])\n",
        "num_classes = df[\"label\"].nunique()\n",
        "\n",
        "\n",
        "\n",
        "#Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")  # 預設 tokenizer，可換\n",
        "encodings = tokenizer(df['content'].tolist(), truncation=True, padding='max_length', max_length=128)\n",
        "df['input_ids'] = encodings['input_ids']\n",
        "df['attention_mask'] = encodings['attention_mask']\n",
        "\n",
        "#Dataset\n",
        "dataset = CustomDataset(df)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset)-train_size])\n",
        "\n",
        "train_labels = [train_dataset[i]['labels'].item() for i in range(len(train_dataset))]\n",
        "class_counts = pd.Series(train_labels).value_counts().to_dict()\n",
        "weights = [1.0 / class_counts[label] for label in train_labels]\n",
        "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "#模型定義\n",
        "model_variants = {\n",
        "    \"FusionMacBERT\": FusionMacBERTModel(\"hfl/chinese-macbert-base\", len(num_cols), num_classes),\n",
        "    \"PureMacBERT\": PureMacBERTModel(\"hfl/chinese-macbert-base\", num_classes),\n",
        "    \"NumericOnly\": NumericOnlyModel(len(num_cols), num_classes),\n",
        "    \"BiLSTMWithNumeric\": BiLSTMWithNumeric(tokenizer.vocab_size, 128, 128, len(num_cols), num_classes),\n",
        "    \"MacBERTWithGRU\": MacBERTWithGRU(\"hfl/chinese-macbert-base\", len(num_cols), num_classes),\n",
        "    \"MacBERTMLPFusion\": MacBERTMLPFusion(\"hfl/chinese-macbert-base\", len(num_cols), num_classes),\n",
        "    \"TextCNNMacBERT\": TextCNNMacBERT(\"hfl/chinese-macbert-base\", len(num_cols), num_classes),\n",
        "    \"RoBERTa\": FusionMacBERTModel(\"hfl/chinese-roberta-wwm-ext\", len(num_cols), num_classes),\n",
        "    \"BERTwwmExt\": FusionMacBERTModel(\"hfl/chinese-bert-wwm-ext\", len(num_cols), num_classes),\n",
        "    \"ERNIE\": FusionMacBERTModel(\"nghuyong/ernie-3.0-base-zh\", len(num_cols), num_classes),\n",
        "    \"ConvBERT\": FusionMacBERTModel(\"YituTech/conv-bert-base\", len(num_cols), num_classes)\n",
        "}\n",
        "\n",
        "#模型訓練\n",
        "for name, model in model_variants.items():\n",
        "    print(f\"訓練模型: {name}\")\n",
        "    train_and_eval(model, name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wqejxS910SJ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# 資料分割：資料集切分與取樣\n",
        "dataset = CustomDataset(df)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset)-train_size])\n",
        "train_labels = [train_dataset[i]['labels'].item() for i in range(len(train_dataset))]\n",
        "class_counts = pd.Series(train_labels).value_counts().to_dict()\n",
        "weights = [1.0 / class_counts[label] for label in train_labels]\n",
        "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "# 執行多模型訓練\n",
        "model_variants = {\n",
        "    #\"FusionMacRegressor\": FusionMacBERTRegressor(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
        "    \"FusionMacBERT\": FusionMacBERTModel(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
        "    \"PureMacBERT\": PureMacBERTModel(\"hfl/chinese-macbert-base\", 3),\n",
        "    \"NumericOnly\": NumericOnlyModel(len(num_cols), 3),\n",
        "    \"BiLSTMWithNumeric\": BiLSTMWithNumeric(tokenizer.vocab_size, 128, 128, len(num_cols), 3),\n",
        "    \"MacBERTWithGRU\": MacBERTWithGRU(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
        "    \"MacBERTMLPFusion\": MacBERTMLPFusion(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
        "    \"TextCNNMacBERT\": TextCNNMacBERT(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
        "    \"RoBERTa\": FusionMacBERTModel(\"hfl/chinese-roberta-wwm-ext\", len(num_cols), 3),\n",
        "    \"BERTwwmExt\": FusionMacBERTModel(\"hfl/chinese-bert-wwm-ext\", len(num_cols), 3),\n",
        "    \"ERNIE\": FusionMacBERTModel(\"nghuyong/ernie-3.0-base-zh\", len(num_cols), 3),\n",
        "    \"ConvBERT\": FusionMacBERTModel(\"YituTech/conv-bert-base\", len(num_cols), 3)\n",
        "}\n",
        "\n",
        "# 逐個模型訓練與輸出結果\n",
        "for name, model in model_variants.items():\n",
        "    tokenizer_name = model_tokenizer_map.get(name, default_tokenizer_name)\n",
        "    if tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "        encodings = tokenizer(df['content'].tolist(), truncation=True, padding='max_length', max_length=128)\n",
        "        df['input_ids'] = encodings['input_ids']\n",
        "        df['attention_mask'] = encodings['attention_mask']\n",
        "    train_and_eval(model, name)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WP2MxFtLUhR"
      },
      "source": [
        "1. FusionMacBERT ✅文字 + ✅數值\n",
        "BERT： 使用 MacBERT\n",
        "架構： 把 [CLS] 向量與數值特徵經過 MLP 融合\n",
        "用途： 做為 baseline 融合模型\n",
        "優點： 同時考慮內容語義與貼文統計資料（如按讚數、是否有 hashtag）\n",
        "\n",
        "2. PureMacBERT ✅文字 + ❌數值\n",
        "BERT： 使用 MacBERT\n",
        "架構： 單純使用 [CLS]，後接 linear 層分類\n",
        "用途： 純語言模型 baseline\n",
        "對照： 可用來比較是否有數值輔助提升效果\n",
        "\n",
        "3. NumericOnly ❌文字 + ✅數值\n",
        "模型類型： 只有數值輸入，經過 MLP 做分類\n",
        "用途： 測試「只靠貼文統計資料」能否達到合理分類\n",
        "對照： 可與文字模型或融合模型對比效果\n",
        "\n",
        "4. BiLSTMWithNumeric ✅文字（Embedding+LSTM）+ ✅數值\n",
        "嵌入方式： 使用 nn.Embedding + BiLSTM 處理文字（不是 BERT）\n",
        "融合方式： 將 LSTM 最後時間步 + 數值特徵拼接\n",
        "特別點： 測試「非 Transformer 模型」是否仍具競爭力\n",
        "\n",
        "5. MacBERTWithGRU ✅文字（MacBERT）+ ✅數值\n",
        "文字處理： MacBERT 之後再串 GRU\n",
        "融合方式： GRU 輸出最後一步拼接數值特徵\n",
        "意圖： 想看 BERT+RNN 的表現 vs. 傳統 BERT\n",
        "\n",
        "6. MacBERTMLPFusion ✅文字 + ✅數值\n",
        "處理方式： 文字與數值直接拼接後進入 MLP\n",
        "不同於 FusionMacBERT：\n",
        "沒有額外處理數值特徵（如沒有經過 nn.Linear)\n",
        "更單純的融合設計（屬於 Early Fusion）\n",
        "\n",
        "7. TextCNNMacBERT ✅文字 + ✅數值\n",
        "模型組合：\n",
        "使用 BERT 編碼後丟進 CNN filter (TextCNN)\n",
        "再與數值特徵融合\n",
        "用途： 測試 BERT 結合 CNN 特徵提取是否提升效果\n",
        "有趣點： 有些短文模型（如微博、Threads）對 CNN 特徵抓取敏感\n",
        "\n",
        "8. RoBERTa ✅文字 + ✅數值\n",
        "BERT 替代品： 改用 RoBERTa（中文版本）\n",
        "融合方式： 同 FusionMacBERT\n",
        "實驗目的： 測試不同語言模型對結果的影響（語言模型 ablation）\n",
        "\n",
        "\n",
        "9. BERTwwmExt ✅文字 + ✅數值\n",
        "BERT： 使用 Chinese BERT whole-word-masking 擴展版\n",
        "比較目的： 同上，用於測試不同語言模型特性的影響\n",
        "\n",
        "10. ERNIE ✅文字 + ✅數值\n",
        "BERT： 改用百度的 ERNIE（引入知識增強）\n",
        "適用場景： 當文本與常識有關（如話題、用語）\n",
        "目的： 評估知識型語言模型在社群文本分類的效果\n",
        "\n",
        "11. ConvBERT ✅文字 + ✅數值\n",
        "模型特色： 使用 Convolution + Self-Attention 混合架構的 BERT\n",
        "實驗意義： 試驗非傳統 Self-Attention 模型是否有優勢\n",
        "\n",
        "\n",
        "| 模型名稱              | 說明               | 是否融合 | 文本處理法         | 特殊處理       |\n",
        "| ----------------- | ---------------- | ---- | ------------- | ---------- |\n",
        "| FusionMacBERT     | BERT + 數值特徵      | ✅    | MacBERT       | 自製融合層      |\n",
        "| PureMacBERT       | 純文本模型            | ❌    | MacBERT       | baseline   |\n",
        "| NumericOnly       | 純統計數值            | ❌    | 無             | MLP only   |\n",
        "| BiLSTMWithNumeric | LSTM + 數值        | ✅    | nn.Embedding  | 不使用 BERT   |\n",
        "| MacBERTWithGRU    | BERT + GRU + 數值  | ✅    | MacBERT + GRU | 時序特徵強化     |\n",
        "| MacBERTMLPFusion  | BERT + 數值        | ✅    | MacBERT       | 拼接後進 MLP   |\n",
        "| TextCNNMacBERT    | BERT + CNN + 數值  | ✅    | MacBERT + CNN | 模仿 TextCNN |\n",
        "| RoBERTa           | 換 BERT backbone  | ✅    | RoBERTa       | 模型比較       |\n",
        "| BERTwwmExt        | 換 BERT backbone  | ✅    | BERT-wwm      | 模型比較       |\n",
        "| ERNIE             | 引入知識的 BERT       | ✅    | ERNIE         | 模型比較       |\n",
        "| ConvBERT          | 混合卷積 + 注意力的 BERT | ✅    | ConvBERT      | 模型比較       |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBCtDWKlz6m8"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "# 資料分割：資料集切分與取樣\n",
        "dataset = CustomDataset(df)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset)-train_size])\n",
        "train_labels = [train_dataset[i]['labels'].item() for i in range(len(train_dataset))]\n",
        "class_counts = pd.Series(train_labels).value_counts().to_dict()\n",
        "weights = [1.0 / class_counts[label] for label in train_labels]\n",
        "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "# 執行多模型訓練\n",
        "model_variants = {\n",
        "    \"FusionMacBERT\": FusionMacBERTModel(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
        "    \"PureMacBERT\": PureMacBERTModel(\"hfl/chinese-macbert-base\", 3),\n",
        "    \"NumericOnly\": NumericOnlyModel(len(num_cols), 3),\n",
        "    \"BiLSTMWithNumeric\": BiLSTMWithNumeric(tokenizer.vocab_size, 128, 128, len(num_cols), 3),\n",
        "    \"MacBERTWithGRU\": MacBERTWithGRU(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
        "    \"MacBERTMLPFusion\": MacBERTMLPFusion(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
        "    \"TextCNNMacBERT\": TextCNNMacBERT(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
        "    \"RoBERTa\": FusionMacBERTModel(\"hfl/chinese-roberta-wwm-ext\", len(num_cols), 3),\n",
        "    \"BERTwwmExt\": FusionMacBERTModel(\"hfl/chinese-bert-wwm-ext\", len(num_cols), 3),\n",
        "    \"ERNIE\": FusionMacBERTModel(\"nghuyong/ernie-3.0-base-zh\", len(num_cols), 3),\n",
        "    \"ConvBERT\": FusionMacBERTModel(\"YituTech/conv-bert-base\", len(num_cols), 3)\n",
        "}\n",
        "\n",
        "# 逐個模型訓練與輸出結果\n",
        "for name, model in model_variants.items():\n",
        "    tokenizer_name = model_tokenizer_map.get(name, default_tokenizer_name)\n",
        "    if tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "        encodings = tokenizer(df['content'].tolist(), truncation=True, padding='max_length', max_length=128)\n",
        "        df['input_ids'] = encodings['input_ids']\n",
        "        df['attention_mask'] = encodings['attention_mask']\n",
        "    train_and_eval(model, name)\n",
        " '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sd8HLHR6VL1"
      },
      "source": [
        "# 迴歸預測"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "291iyZhHxpj9"
      },
      "outputs": [],
      "source": [
        "# 原本這樣分類（要拿掉）\n",
        "# df['view_class'] = ...\n",
        "# df['label'] = ...\n",
        "\n",
        "# 直接用原始 view_count 作為 regression target\n",
        "df = df.dropna(subset=[\"content\", \"view_count\"])\n",
        "df['target'] = df['view_count'].apply(parse_count)  # 如果 view_count 不是數字要先轉換\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dedliSNv7UGU"
      },
      "outputs": [],
      "source": [
        "def train_and_eval_regression(model, name):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # 訓練\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            numerics = batch['numerics'].to(device)\n",
        "            targets = batch['labels'].float().to(device)  # 重要：labels 必須是 float\n",
        "            preds = model(input_ids, attention_mask, numerics)\n",
        "            loss = loss_fn(preds, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # 評估\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            numerics = batch['numerics'].to(device)\n",
        "            targets = batch['labels'].float().to(device)\n",
        "            preds = model(input_ids, attention_mask, numerics)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "    mse = mean_squared_error(all_targets, all_preds)\n",
        "    mae = mean_absolute_error(all_targets, all_preds)\n",
        "    r2 = r2_score(all_targets, all_preds)\n",
        "    print(f\"\\n{name}  MSE: {mse:.2f} | MAE: {mae:.2f}| R2: {r2:.2f}\")\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "    return all_targets, all_preds\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c7CQi1v6wwI"
      },
      "outputs": [],
      "source": [
        "class FusionMacBERTRegressor(nn.Module):\n",
        "    def __init__(self, model_name, num_numeric_features, output_dim=1):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
        "        self.regressor = nn.Linear(self.bert.config.hidden_size + 64, output_dim)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, numerics):\n",
        "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
        "        num_out = torch.relu(self.numeric_fc(numerics))\n",
        "        combined = torch.cat((cls_output, num_out), dim=1)\n",
        "        return self.regressor(self.dropout(combined)).squeeze(1)  # (batch,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTncOradCUz8"
      },
      "outputs": [],
      "source": [
        "rm -rf ~/.cache/huggingface/transformers/hfl__chinese-macbert-base\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptY9GnoACUz8"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "'''\n",
        "class CustomDatasetRegression(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.input_ids = df['input_ids'].tolist()\n",
        "        self.attention_mask = df['attention_mask'].tolist()\n",
        "        self.labels = df['label'].astype(float).values   # ← 為回歸任務需轉成 float\n",
        "        self.numerics = df[num_cols].astype(float).values  # ← 確保為 float array\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
        "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),  # ← 修正為 float\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)       # ← 修正為 float\n",
        "        }\n",
        "'''\n",
        "class CustomDatasetRegression(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.input_ids = df['input_ids'].tolist()\n",
        "        self.attention_mask = df['attention_mask'].tolist()\n",
        "        self.labels = df['target'].astype(float).values\n",
        "        self.numerics = df[num_cols].astype(float).values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
        "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        logp = self.ce(input, target)\n",
        "        p = torch.exp(-logp)\n",
        "        loss = (1 - p) ** self.gamma * logp\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rlezwnry2K0h"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = CustomDatasetRegression(df)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "\n",
        "class FusionMacBERTRegressor(nn.Module):\n",
        "    def __init__(self, model_name, num_numeric_features, output_dim=1):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
        "        self.regressor = nn.Linear(self.bert.config.hidden_size + 64, output_dim)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, numerics):\n",
        "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
        "        num_out = torch.relu(self.numeric_fc(numerics))\n",
        "        combined = torch.cat((cls_output, num_out), dim=1)\n",
        "        return self.regressor(self.dropout(combined)).squeeze(1)\n",
        "\n",
        "\n",
        "def train_and_eval_regression(model, name):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            numerics = batch['numerics'].to(device)\n",
        "            targets = batch['labels'].float().to(device)\n",
        "\n",
        "            preds = model(input_ids, attention_mask, numerics)\n",
        "            loss = loss_fn(preds, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            numerics = batch['numerics'].to(device)\n",
        "            targets = batch['labels'].float().to(device)\n",
        "\n",
        "            preds = model(input_ids, attention_mask, numerics)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "    mse = mean_squared_error(all_targets, all_preds)\n",
        "    mae = mean_absolute_error(all_targets, all_preds)\n",
        "    r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "    print(f\"\\n{name}  MSE: {mse:.2f} | MAE: {mae:.2f} | R²: {r2:.2f}\")\n",
        "    return all_targets, all_preds\n",
        "\n",
        "\n",
        "model = FusionMacBERTRegressor(\"hfl/chinese-macbert-base\", len(num_cols))\n",
        "all_targets, all_preds = train_and_eval_regression(model, \"FusionMacBERTRegressor\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQwvt6LQ10Pi"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF-tbCHU10Ea"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmUJQfroFBqt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEyKqKjE9oZy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OeqnHopFJuT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0DyH1z7FJm3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnoabF2fFJgR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaucnuMEFCV-"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvuTn85bUTA5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nzhynZN7IYy"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTn9buro9xlD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjsKVnf0-iiU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMJI_upxwRgP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gbdm9r7_qFq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTk9I4ZtEPBP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzB1yP1GFFki"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac0a2a3c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bb2b24c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae6de597"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#person_view\n",
        "df[\"person_view_encoded\"] = LabelEncoder().fit_transform(df[\"person_view\"])\n",
        "\n",
        "#特徵變數與目標變數\n",
        "features = [\"like_count\", \"reply_count\", \"emoji_count\", \"sentiment_score\", \"person_view_encoded\"]\n",
        "X = df[features]\n",
        "y = df[\"viral\"]\n",
        "\n",
        "#分割資料\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#評估模型\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
