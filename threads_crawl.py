from selenium import webdriver
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
from datetime import datetime, timezone
import time
import re
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import os
from openpyxl import load_workbook, Workbook
def save_post_to_excel(post_data, account):
    file_name = "threads.xlsx"

    if not os.path.exists(file_name):
        # å¦‚æœ Excel ä¸å­˜åœ¨ï¼Œå»ºç«‹ä¸€å€‹æ–°çš„
        wb = Workbook()
        ws = wb.active
        ws.title = account
        ws.append(list(post_data.keys()))  # å¯«å…¥æ¬„ä½åç¨±
        ws.append(list(post_data.values()))
        wb.save(file_name)
        print(f"âœ… å»ºç«‹æ–°æª”æ¡ˆä¸¦æ–°å¢ä¸€ç­†è³‡æ–™åˆ° {file_name}")
    else:
        # å¦‚æœå­˜åœ¨ï¼Œè®€å–å¾Œè¿½åŠ 
        wb = load_workbook(file_name)
        if account in wb.sheetnames:
            ws = wb[account]
        else:
            ws = wb.create_sheet(title=account)
            ws.append(list(post_data.keys()))  # æ–° sheet è¦å¯«æ¬„ä½åç¨±

        ws.append(list(post_data.values()))
        wb.save(file_name)
        print(f"âœ… å·²æ–°å¢ä¸€ç­†è³‡æ–™åˆ° {file_name} çš„ {account} å·¥ä½œè¡¨")

def scrape_threads(account, password, target_post_count):
    results = []
    collected_posts = 0
    scroll_count = 0
    max_scrolls = 30

    options = Options()
    driver = webdriver.Chrome(options=options)

    try:
        driver.get("https://www.threads.net/login")
        time.sleep(5)

        username_input = driver.find_element(By.CSS_SELECTOR, 'input[autocomplete="username"]')
        username_input.send_keys(account)

        password_input = driver.find_element(By.CSS_SELECTOR, 'input[autocomplete="current-password"]')
        password_input.send_keys(password)

        login_button = driver.find_element(By.XPATH, '//div[text()="ç™»å…¥"]/ancestor::div[@role="button"]')
        login_button.click()
        time.sleep(20)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "time")))


        while collected_posts < target_post_count and scroll_count < max_scrolls:
            current_post_blocks = driver.find_elements(By.XPATH, '//div[@data-pressable-container="true"]')

            if len(current_post_blocks) > collected_posts:
                for idx, post in enumerate(current_post_blocks[collected_posts:], collected_posts + 1):
                    soup = BeautifulSoup(post.get_attribute("innerHTML"), "html.parser")

                    author_tag = soup.find("a", href=lambda x: x and x.startswith("/@"))
                    author = author_tag.get_text(strip=True) if author_tag else "æœªçŸ¥"

                    time_tag = soup.find("time")
                    if time_tag and time_tag.has_attr("title"):
                        try:
                            time_title = time_tag["title"]
                            if "å¹´" in time_title:
                                year = int(re.search(r'(\\d+)å¹´', time_title).group(1))
                                month = int(re.search(r'(\\d+)æœˆ', time_title).group(1))
                                day = int(re.search(r'(\\d+)æ—¥', time_title).group(1))
                                time_part = re.search(r'(ä¸Šåˆ|ä¸‹åˆ)(\\d+):(\\d+)', time_title)
                                if time_part:
                                    am_pm = time_part.group(1)
                                    hour = int(time_part.group(2))
                                    minute = int(time_part.group(3))
                                    if am_pm == "ä¸‹åˆ" and hour < 12:
                                        hour += 12
                                    elif am_pm == "ä¸Šåˆ" and hour == 12:
                                        hour = 0
                                    post_time = f"{year}å¹´{month:02d}æœˆ{day:02d}æ—¥ {hour:02d}:{minute:02d}"
                                else:
                                    post_time = "æœªçŸ¥"
                            else:
                                parsed_date = datetime.strptime(time_title, "%A, %B %d, %Y at %I:%M %p")
                                post_time = parsed_date.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
                        except:
                            post_time = time_tag["datetime"] if time_tag.has_attr("datetime") else "æœªçŸ¥"
                    else:
                        post_time = "æœªçŸ¥"

                    photo, video = check_media_types(soup)
                    content_spans = soup.select('span[dir=\"auto\"]')
                    content = " ".join([s.get_text(strip=True) for s in content_spans if s.get_text(strip=True)])
                    result = extract_post_info(content)

                    detail_a_tag = soup.find("a", href=lambda x: x and "/post/" in x)
                    if detail_a_tag:
                        post_url = f"https://www.threads.net{detail_a_tag['href']}"
                        stats = open_post_detail(driver, post_url)

                        single_post = {
                            "author": author,
                            "post_time": post_time,
                            "topic": result["topic"],
                            "time_info": result["time"],
                            "content": result["content"],
                            "has_photo": photo,
                            "has_video": video,
                            "like_count": stats["like_count"],
                            "reply_count": stats["reply_count"],
                            "repost_count": stats["repost_count"],
                            "share_count": stats["share_count"],
                            "view_count": stats["view_count"],
                            "followers_count": stats["followers_count"],
                            "post_url": post_url,
                            "scrape_time": datetime.now().isoformat()
                        }

                        # æ–°å¢çµæœ
                        results.append(single_post)
                        # ç«‹åˆ»å¯«å…¥ Excel
                        save_post_to_excel(single_post, account)
                    else:
                        print("âŒ æ‰¾ä¸åˆ°è©³ç´°è²¼æ–‡é€£çµï¼Œç•¥é")

                    time.sleep(3)

            collected_posts = len(current_post_blocks)
            if collected_posts >= target_post_count:
                break

            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            print("å‘ä¸‹æ»¾å‹•é é¢...")
            time.sleep(5)

            try:
                previous_count = collected_posts
                WebDriverWait(driver, 10).until(
                    lambda d: len(d.find_elements(By.XPATH, '//div[@data-pressable-container=\"true\"]')) > previous_count
                )
            except:
                print("æ»¾å‹•å¾Œæ²’æœ‰åŠ è¼‰æ–°è²¼æ–‡")
            scroll_count += 1

        print(f"ğŸ¯ å®Œæˆï¼Œç¸½å…±æ”¶é›† {collected_posts} ç­†è²¼æ–‡")

        return results

    finally:
        driver.quit()

def extract_post_info(raw_text: str):
    """å¾è²¼æ–‡å…§å®¹ä¸­æå–è³‡è¨Š"""
    # åˆªé™¤ã€Œç¿»è­¯ã€èˆ‡çµå°¾æ•¸å­—
    raw_text = re.sub(r"ç¿»è­¯.*?$", "", raw_text)
    raw_text = re.sub(r"\d+\s+\d*\s*\d*\s*\d*\s*$", "", raw_text).strip()

    parts = raw_text.split()

    if len(parts) < 3:
        return {"author": "", "topic": "", "time": "", "content": ""}

    # æ‰¾å‡ºæ™‚é–“æ¬„ä½ï¼ˆä»¥ã€Œåˆ†é˜ã€ã€ã€Œå°æ™‚ã€ã€ã€Œå¤©ã€ç­‰å–®ä½åˆ¤æ–·ï¼‰
    time_index = next((i for i, word in enumerate(parts) if re.search(r'(åˆ†é˜|å°æ™‚|å¤©)$', word)), None)

    if time_index is None or time_index < 1:
        return {"author": "", "topic": "", "time": "", "content": ""}

    author = parts[0]
    
    # ä¸»é¡Œæ˜¯å¾ä½œè€…å¾Œåˆ°æ™‚é–“å‰çš„æ‰€æœ‰å…§å®¹
    topic = " ".join(parts[1:time_index])
    
    time_str = parts[time_index]
    
    # å…§å®¹æ˜¯æ™‚é–“æ¬„ä½ä¹‹å¾Œçš„æ‰€æœ‰æ–‡å­—
    content = " ".join(parts[time_index + 1:])

    return {
        "author": author,
        "topic": topic,
        "time": time_str,
        "content": content
    }

def check_media_types(soup):
    """æª¢æŸ¥è²¼æ–‡æ˜¯å¦åŒ…å«åœ–ç‰‡æˆ–å½±ç‰‡"""
    photo = "N"
    video = "N"

    # æª¢æŸ¥åœ–ç‰‡ï¼ˆæ’é™¤å¤§é ­è²¼ï¼‰
    img_tags = soup.find_all("img")
    for img in img_tags:
        alt = img.get("alt", "")
        if "å¤§é ­è²¼ç…§" not in alt:
            photo = "Y"
            break

    # æª¢æŸ¥å½±ç‰‡
    if soup.find("video"):
        video = "Y"

    return photo, video

def open_post_detail(driver, url: str):
    """é–‹å•Ÿè²¼æ–‡è©³æƒ…é é¢ä¸¦ç²å–çµ±è¨ˆè³‡è¨Š"""
    # é–‹æ–°åˆ†é 
    driver.execute_script("window.open(arguments[0], '_blank');", url)
    driver.switch_to.window(driver.window_handles[1])
    time.sleep(5)

    # BeautifulSoup è™•ç†é é¢
    soup = BeautifulSoup(driver.page_source, "html.parser")

    # æ“·å–æ•¸æ“šçµ±è¨ˆæ•¸å­—
    def get_stat_by_index(index):
        spans = soup.find_all("span", class_=re.compile("x17qophe"))
        try:
            return spans[index].get_text(strip=True)
        except IndexError:
            return "0"

    like_count = get_stat_by_index(0)
    reply_count = get_stat_by_index(1)
    repost_count = get_stat_by_index(2)
    share_count = get_stat_by_index(3)

    print(f"â¤ï¸ æŒ‰è®šæ•¸ï¼š{like_count}")
    print(f"ğŸ’¬ ç•™è¨€æ•¸ï¼š{reply_count}")
    print(f"ğŸ” è½‰ç™¼æ•¸ï¼š{repost_count}")
    print(f"ğŸ“¤ åˆ†äº«æ•¸ï¼š{share_count}")

    # çˆ¬å–ã€Œç€è¦½æ¬¡æ•¸ã€- æ ¹æ“šæ–°æä¾›çš„HTMLçµæ§‹
    try:
        # å°‹æ‰¾åŒ…å«ã€Œæ¬¡ç€è¦½ã€æ–‡å­—çš„span
        view_count_span = soup.find("span", string=lambda s: s and ("æ¬¡ç€è¦½" in s))
        
        if view_count_span:
            # æå–ç€è¦½æ¬¡æ•¸ï¼ˆç§»é™¤ã€Œæ¬¡ç€è¦½ã€æ–‡å­—ï¼‰
            view_count_text = view_count_span.get_text(strip=True)
            view_count = view_count_text.replace("æ¬¡ç€è¦½", "")
        else:
            # å‚™ç”¨æ–¹æ³•ï¼šå°‹æ‰¾ç‰¹å®šçµæ§‹
            views_div = soup.find("div", class_=lambda c: c and "x6s0dn4" in c and "xfex06f" in c)
            if views_div:
                view_span = views_div.find("span", string=lambda s: s and ("æ¬¡ç€è¦½" in s or "views" in s.lower()))
                if view_span:
                    view_count_text = view_span.get_text(strip=True)
                    view_count = view_count_text.replace("æ¬¡ç€è¦½", "").replace("views", "").strip()
                else:
                    view_count = "æœªçŸ¥"
            else:
                view_count = "æœªçŸ¥"
        
        print("ğŸ‘ï¸ è§€çœ‹æ¬¡æ•¸ï¼š", view_count)
    except Exception as e:
        print("âš ï¸ è§€çœ‹æ¬¡æ•¸è§£æå¤±æ•—ï¼š", e)
        view_count = "0"
        
    try:
        time_tag = soup.find("time")
        author_href, author_name = "æœªçŸ¥", "æœªçŸ¥"
        author_username = "æœªçŸ¥"

        if time_tag:
            # æ‰¾åˆ° time æ‰€åœ¨å€å¡Šçš„çˆ¶å±¤ï¼ˆä¾‹å¦‚å« post metadata çš„å€å¡Šï¼‰
            block = time_tag.find_parent("div")
            if block:
                # åœ¨é€™å€å¡Šå…§å°‹æ‰¾ <a href="/@xxx">
                author_link = block.find("a", href=re.compile(r"^/@"), attrs={"role": "link"})
                if author_link:
                    author_href = author_link["href"]
                    author_username = author_href.split("/")[1].lstrip("@")
        print(f"âœï¸ ç™¼æ–‡è€…å¸³è™Ÿï¼š{author_username}")
        print(f"ğŸ”— å€‹äººé€£çµï¼šhttps://www.threads.net/@{author_username}")
    except Exception as e:
        print("âš ï¸ ç™¼æ–‡è€…è§£æå¤±æ•—ï¼š", e)
        
    followers_count = "0"
    try:
        # åœ¨æ–°åˆ†é ä¸­æ‰“é–‹ç™¼æ–‡è€…çš„ä¸»é 
        driver.execute_script("window.open(arguments[0], '_blank');", f"https://www.threads.net/@{author_username}")
        
        # åˆ‡æ›åˆ°ç™¼æ–‡è€…ä¸»é åˆ†é ï¼ˆwindow_handles[2]ï¼Œå› ç‚ºç•¶å‰æ˜¯[1]ï¼‰
        driver.switch_to.window(driver.window_handles[2])
        
        # ç­‰å¾…é é¢åŠ è¼‰
        time.sleep(5)
        
        # æŠ“å–è¿½è¹¤è€…æ•¸é‡
        followers_count = get_followers_count(driver)
        print(f"ğŸ‘¥ è¿½è¹¤è€…æ•¸é‡ï¼š{followers_count}")
        
        # é—œé–‰ç™¼æ–‡è€…ä¸»é åˆ†é 
        driver.close()
        
        # å›åˆ°è²¼æ–‡åˆ†é 
        driver.switch_to.window(driver.window_handles[1])
        
    except Exception as e:
        print(f"âš ï¸ æŠ“å–è¿½è¹¤è€…æ•¸é‡æ™‚å‡ºéŒ¯ï¼š{e}")
    
    # é—œé–‰è²¼æ–‡é ä¸¦å›åˆ°ä¸»åˆ†é 
    driver.close()
    driver.switch_to.window(driver.window_handles[0])
    time.sleep(1)
    
    return {
        "like_count": like_count,
        "reply_count": reply_count,
        "repost_count": repost_count,
        "share_count": share_count,
        "view_count": view_count,
        "followers_count": followers_count
    }

def get_followers_count(driver, soup=None):
    """
    å¾ç›®å‰é é¢æŠ“å–è¿½è¹¤è€…æ•¸é‡ï¼ˆæ”¯æ´ä¸­è‹±æ–‡ï¼‰

    :param driver: WebDriver å¯¦ä¾‹
    :param soup: å¯é¸ï¼Œå·²è§£æçš„ BeautifulSoup ç‰©ä»¶ã€‚å¦‚æœæ²’æä¾›æœƒå¾ driver.page_source å‰µå»º
    :return: è¿½è¹¤è€…æ•¸é‡å­—ä¸²
    """
    try:
        if soup is None:
            soup = BeautifulSoup(driver.page_source, 'html.parser')

        span_tags = soup.find_all("span")

        for span in span_tags:
            text = span.get_text(strip=True)
            # æ”¯æ´ "followers" æˆ– "ä½ç²‰çµ²"
            if "followers" in text.lower() or "ä½ç²‰çµ²" in text:
                inner_span = span.find("span")
                if inner_span and inner_span.has_attr("title"):
                    return inner_span["title"].replace(",", "")
                else:
                    match = re.search(r"(\d+(?:,\d+)*)", text)
                    if match:
                        return match.group(1).replace(",", "")
        
        return "0"

    except Exception as e:
        print(f"âš ï¸ æŠ“å–è¿½è¹¤è€…æ•¸é‡æ™‚å‡ºéŒ¯ï¼š{e}")
        return "0"

accounts = []

if __name__ == "__main__":
        account = ""
        target_post_count = 400
        
        scrape_threads(account, "", target_post_count)
